{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_adapters import StableDiffusionPipelineAdapterEmbeddings, StableDiffusionXLPipelineAdapterEmbeddings\n",
    "from prior_models import TransformerEmbeddingDiffusionModelv2\n",
    "from eval_utils import compute_embeddings\n",
    "from utils import map_embeddings_to_ratings, split_recommender_data, store_eval_images, store_eval_images_per_user\n",
    "\n",
    "from diffusers import DDPMScheduler\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from diffusers.utils import load_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from diffusion_adapters import StableDiffusionPipelineAdapterEmbeddings\n",
    "from sampling import sample_from_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=6000)\n",
    "device = \"cuda\"\n",
    "users = 94\n",
    "images_per_user = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\.cache\\huggingface\\hub\\models--runwayml--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798b98ee756449c2bdb28e1238645db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diffusion_prior_model = TransformerEmbeddingDiffusionModelv2(\n",
    "    img_embed_dim=1024,\n",
    "    num_users=94,    # So user embedding covers your entire user set\n",
    "    n_heads=16,\n",
    "    num_tokens=1,\n",
    "    num_user_tokens=4,\n",
    "    num_layers=8,\n",
    "    dim_feedforward=2048,\n",
    "    whether_use_user_embeddings=True\n",
    ").to(device)\n",
    "\n",
    "savepath = \"../data/flickr/evaluation/diffusion_priors/models/weights/sd15_nl8_heads16_dim_feedforward2048_lr0.0001_it1_ut4_adamw_reduce_on_plateau_bs64_nslinear_spu80_timesteps6000_objnoise-pred_useueTrue.pth\"\n",
    "llm_savepath = \"../data/flickr/evaluation/baselines/llm_profiling/generated_images/per_user/\"\n",
    "T2_savepath = \"../data/flickr/evaluation/baselines/t2_prompt/\"\n",
    "gt_savepath = \"../data/flickr/evaluation/ground_truth/usrthrs_100/liked_per_user\"\n",
    "\n",
    "diffusion_prior_model.load_state_dict(torch.load(savepath, weights_only=True))\n",
    "diffusion_prior_model.eval()\n",
    "\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"                                                                                                                                                                                                                \n",
    "pipe = StableDiffusionPipelineAdapterEmbeddings.from_pretrained(model_id).to(\"cuda\")\n",
    "pipe.safety_checker = None\n",
    "pipe.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\") \n",
    "\n",
    "t2_embeddings = torch.load(f\"{T2_savepath}/embeddings/sd15_embeddings.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_precision_recall_at_k(\n",
    "    generated,  # (n, latent_dim)\n",
    "    encoded_real,  # (N, latent_dim)\n",
    "    R,  # (N, num_users)\n",
    "    top_k=1\n",
    "):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    sims = cosine_similarity(generated.cpu().numpy(), encoded_real.cpu().numpy())  # (n, N)\n",
    "    topk_indices = sims.argsort(axis=1)[:, -top_k:]  # (N, n)\n",
    "    recommended_items = np.unique(topk_indices.flatten())\n",
    "    liked_items = np.where(R == 1)[0]\n",
    "    relevant_retrieved = np.intersect1d(recommended_items, liked_items)\n",
    "    precision = len(relevant_retrieved) / len(recommended_items)\n",
    "    recall = len(relevant_retrieved) / len(liked_items)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 21.85it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.83it/s]]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.67it/s]]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.43it/s]]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.22it/s]]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.98it/s]]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.48it/s]]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.92it/s]]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.43it/s]]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.23it/s]]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.71it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.65it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.98it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.22it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.34it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.26it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.05it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.80it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.27it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.07it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.34it/s]  \n",
      "100%|██████████| 50/50 [00:02<00:00, 21.39it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.24it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.92it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.30it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.05it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.14it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.41it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.90it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.80it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.90it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.56it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.27it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.21it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.59it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.98it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.22it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.94it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.19it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.64it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.50it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.22it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.46it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.84it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.19it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.97it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.55it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.94it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.71it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.77it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.50it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.95it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.61it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.62it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.86it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.67it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.73it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.51it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.15it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.10it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.33it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.12it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.63it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.34it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.29it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.93it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.45it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.53it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.07it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.13it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.38it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.33it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.01it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.60it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.58it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.69it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.39it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.32it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.98it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.74it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.43it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.43it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.30it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.46it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.14it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.82it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.95it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.67it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.79it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.35it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.35it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.01it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.75it/s]t]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.57it/s]t]\n",
      "100%|██████████| 94/94 [1:20:15<00:00, 51.23s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "data = dict.fromkeys(range(users))\n",
    "for user_idx in tqdm(range(users)):\n",
    "    user_dict = dict.fromkeys([\"rebeca_embeddings\", \"llm_embeddings\", \"T2\"])\n",
    "    like = 1\n",
    "    score_tensor = torch.tensor(like).expand(images_per_user).long().to(device)\n",
    "    user_tensor = torch.tensor(user_idx).expand(images_per_user).to(device)\n",
    "    user_ids_uncond_tensor = torch.full_like(user_tensor, fill_value = 94).to(device)\n",
    "    score_uncond_tensor = torch.full_like(score_tensor, fill_value = 2).to(device)\n",
    "\n",
    "    sampled_img_embs = sample_from_diffusion(\n",
    "                model=diffusion_prior_model,\n",
    "                user_ids_cond=user_tensor,\n",
    "                scores_cond=score_tensor,\n",
    "                user_ids_uncond=user_ids_uncond_tensor,\n",
    "                scores_uncond=score_uncond_tensor,\n",
    "                img_embedding_size=1024,\n",
    "                scheduler=noise_scheduler,\n",
    "                guidance_scale=10,\n",
    "                prediction_type=\"epsilon\",\n",
    "                device=\"cuda\",\n",
    "            ).detach()\n",
    "    \n",
    "    llm_path_images = f\"{llm_savepath}/{user_idx}\"\n",
    "    llm_embs = []\n",
    "    with torch.no_grad():\n",
    "        for unq_img_path in tqdm(os.listdir(llm_path_images)):\n",
    "            path = os.path.join(llm_path_images, unq_img_path)\n",
    "            pil_image = load_image(path)\n",
    "            image_emb = pipe.encode_image(pil_image, device=\"cuda\", num_images_per_prompt=1)[0].squeeze()\n",
    "            llm_embs.append(image_emb.cpu())\n",
    "    llm_embs_tensor = torch.stack(llm_embs)[torch.randperm(50)[:images_per_user]]\n",
    "\n",
    "    user_dict[\"rebeca_embeddings\"] = sampled_img_embs.detach().cpu()\n",
    "    user_dict[\"llm_embeddings\"] = llm_embs_tensor\n",
    "    user_dict[\"T2\"] = t2_embeddings[torch.randperm(t2_embeddings.size(0))[:images_per_user]]\n",
    "    data[user_idx]  = user_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1024])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"llm_embeddings\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisions:  0.125 0.0 0.2\n",
      "Recalls:  1.0 0.0 1.0\n",
      "Precisions:  0.7142857142857143 0.6 0.6\n",
      "Recalls:  0.7142857142857143 0.42857142857142855 0.42857142857142855\n",
      "Precisions:  0.25 0.42857142857142855 0.3333333333333333\n",
      "Recalls:  0.6666666666666666 1.0 0.6666666666666666\n",
      "Precisions:  0.4444444444444444 1.0 0.75\n",
      "Recalls:  1.0 0.5 0.75\n",
      "Precisions:  0.125 0.0 0.0\n",
      "Recalls:  1.0 0.0 0.0\n",
      "Precisions:  0.25 0.0 0.3333333333333333\n",
      "Recalls:  0.6666666666666666 0.0 0.6666666666666666\n",
      "Precisions:  0.42857142857142855 1.0 0.6666666666666666\n",
      "Recalls:  0.6 0.8 0.4\n",
      "Precisions:  0.4444444444444444 0.25 0.3333333333333333\n",
      "Recalls:  1.0 0.25 0.5\n",
      "Precisions:  0.5 0.3333333333333333 0.2857142857142857\n",
      "Recalls:  1.0 0.5 0.5\n",
      "Precisions:  0.2222222222222222 0.4 0.3333333333333333\n",
      "Recalls:  1.0 1.0 0.5\n",
      "Precisions:  0.25 0.3333333333333333 0.25\n",
      "Recalls:  1.0 1.0 1.0\n",
      "Precisions:  0.14285714285714285 0.14285714285714285 0.2\n",
      "Recalls:  0.5 0.5 0.5\n",
      "Precisions:  0.7142857142857143 0.4 1.0\n",
      "Recalls:  1.0 0.4 0.8\n",
      "Precisions:  0.1111111111111111 0.0 0.0\n",
      "Recalls:  1.0 0.0 0.0\n",
      "Precisions:  0.5 0.75 0.75\n",
      "Recalls:  0.8 0.6 0.6\n",
      "Precisions:  0.3333333333333333 0.375 0.42857142857142855\n",
      "Recalls:  0.6666666666666666 1.0 1.0\n",
      "Precisions:  0.2222222222222222 0.6666666666666666 0.4\n",
      "Recalls:  1.0 1.0 1.0\n",
      "Precisions:  0.1111111111111111 1.0 0.5\n",
      "Recalls:  0.5 0.5 1.0\n",
      "Precisions:  0.5 0.3333333333333333 0.2\n",
      "Recalls:  0.75 0.25 0.25\n",
      "Precisions:  0.5 0.3333333333333333 0.16666666666666666\n",
      "Recalls:  1.0 0.5 0.25\n",
      "Precisions:  0.14285714285714285 0.3333333333333333 0.3333333333333333\n",
      "Recalls:  0.5 0.5 1.0\n",
      "Precisions:  0.42857142857142855 0.6 0.75\n",
      "Recalls:  0.75 0.75 0.75\n",
      "Precisions:  0.3333333333333333 0.375 0.3333333333333333\n",
      "Recalls:  1.0 1.0 0.6666666666666666\n",
      "Precisions:  0.125 0.375 0.25\n",
      "Recalls:  0.3333333333333333 1.0 0.3333333333333333\n",
      "Precisions:  0.14285714285714285 0.0 0.16666666666666666\n",
      "Recalls:  1.0 0.0 1.0\n",
      "Precisions:  0.375 0.42857142857142855 0.375\n",
      "Recalls:  0.75 0.75 0.75\n",
      "Precisions:  0.625 0.6666666666666666 1.0\n",
      "Recalls:  0.8333333333333334 0.6666666666666666 0.5\n",
      "Precisions:  0.125 0.2 0.0\n",
      "Recalls:  1.0 1.0 0.0\n",
      "Precisions:  0.1111111111111111 0.16666666666666666 0.2\n",
      "Recalls:  1.0 1.0 1.0\n",
      "Precisions:  0.375 0.4 0.4\n",
      "Recalls:  1.0 0.6666666666666666 0.6666666666666666\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m user_gt_embs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/flickr/evaluation/ground_truth/usrthrs_100/users/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/embeddings.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m user_gt_ratings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/flickr/evaluation/ground_truth/usrthrs_100/users/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ratings.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m rebeca_prec, rebeca_rec \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_precision_recall_at_k\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrebeca_embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_real\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_gt_embs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_gt_ratings\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m llm_prec, llm_rec \u001b[38;5;241m=\u001b[39m compute_precision_recall_at_k(\n\u001b[0;32m     15\u001b[0m     generated\u001b[38;5;241m=\u001b[39mdata[user][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     16\u001b[0m     encoded_real\u001b[38;5;241m=\u001b[39muser_gt_embs,\n\u001b[0;32m     17\u001b[0m     R\u001b[38;5;241m=\u001b[39muser_gt_ratings\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m t2_prec, t2_rec \u001b[38;5;241m=\u001b[39m compute_precision_recall_at_k(\n\u001b[0;32m     21\u001b[0m     generated\u001b[38;5;241m=\u001b[39mdata[user][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT2\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     22\u001b[0m     encoded_real\u001b[38;5;241m=\u001b[39muser_gt_embs,\n\u001b[0;32m     23\u001b[0m     R\u001b[38;5;241m=\u001b[39muser_gt_ratings\n\u001b[0;32m     24\u001b[0m )\n",
      "Cell \u001b[1;32mIn[20], line 15\u001b[0m, in \u001b[0;36mcompute_precision_recall_at_k\u001b[1;34m(generated, encoded_real, R, top_k)\u001b[0m\n\u001b[0;32m     13\u001b[0m relevant_retrieved \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mintersect1d(recommended_items, liked_items)\n\u001b[0;32m     14\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(relevant_retrieved) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(recommended_items)\n\u001b[1;32m---> 15\u001b[0m recall \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrelevant_retrieved\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mliked_items\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m precision, recall\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for user in range(94):\n",
    "\n",
    "    user_gt_embs = torch.load(f\"../data/flickr/evaluation/ground_truth/usrthrs_100/users/{user}/embeddings.pt\")\n",
    "    user_gt_ratings = torch.load(f\"../data/flickr/evaluation/ground_truth/usrthrs_100/users/{user}/ratings.pt\")\n",
    "\n",
    "    rebeca_prec, rebeca_rec = compute_precision_recall_at_k(\n",
    "        generated=data[user][\"rebeca_embeddings\"],\n",
    "        encoded_real=user_gt_embs,\n",
    "        R=user_gt_ratings\n",
    "    )\n",
    "\n",
    "    llm_prec, llm_rec = compute_precision_recall_at_k(\n",
    "        generated=data[user][\"llm_embeddings\"],\n",
    "        encoded_real=user_gt_embs,\n",
    "        R=user_gt_ratings\n",
    "    )\n",
    "\n",
    "    t2_prec, t2_rec = compute_precision_recall_at_k(\n",
    "        generated=data[user][\"T2\"],\n",
    "        encoded_real=user_gt_embs,\n",
    "        R=user_gt_ratings\n",
    "    )\n",
    "\n",
    "    print(\"Precisions: \",rebeca_prec, llm_prec, t2_prec)\n",
    "    print(\"Recalls: \", rebeca_rec, llm_rec, t2_rec)\n",
    "\n",
    "data[user][\"rebeca_embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping user 30 due to no positive ratings.\n",
      "Skipping user 35 due to no positive ratings.\n",
      "Skipping user 75 due to no positive ratings.\n",
      "== AVERAGE PRECISIONS ==\n",
      "REBECA: 0.4227280616760254\n",
      "LLM   : 0.5410780310630798\n",
      "T2    : 0.5279958248138428\n",
      "== AVERAGE RECALLS ==\n",
      "REBECA: 0.8911128044128418\n",
      "LLM   : 0.5758023858070374\n",
      "T2    : 0.7311965823173523\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "rebeca_precisions = []\n",
    "rebeca_recalls = []\n",
    "\n",
    "llm_precisions = []\n",
    "llm_recalls = []\n",
    "\n",
    "t2_precisions = []\n",
    "t2_recalls = []\n",
    "\n",
    "for user in range(94):\n",
    "    user_gt_embs = torch.load(f\"../data/flickr/evaluation/ground_truth/usrthrs_100/users/{user}/embeddings.pt\")\n",
    "    user_gt_ratings = torch.load(f\"../data/flickr/evaluation/ground_truth/usrthrs_100/users/{user}/ratings.pt\")\n",
    "\n",
    "    if user_gt_ratings.sum() == 0:\n",
    "        print(f\"Skipping user {user} due to no positive ratings.\")\n",
    "        continue\n",
    "\n",
    "    rebeca_prec, rebeca_rec = compute_precision_recall_at_k(\n",
    "        generated=data[user][\"rebeca_embeddings\"],\n",
    "        encoded_real=user_gt_embs,\n",
    "        R=user_gt_ratings\n",
    "    )\n",
    "    llm_prec, llm_rec = compute_precision_recall_at_k(\n",
    "        generated=data[user][\"llm_embeddings\"],\n",
    "        encoded_real=user_gt_embs,\n",
    "        R=user_gt_ratings\n",
    "    )\n",
    "    t2_prec, t2_rec = compute_precision_recall_at_k(\n",
    "        generated=data[user][\"T2\"],\n",
    "        encoded_real=user_gt_embs,\n",
    "        R=user_gt_ratings\n",
    "    )\n",
    "\n",
    "    rebeca_precisions.append(rebeca_prec)\n",
    "    rebeca_recalls.append(rebeca_rec)\n",
    "\n",
    "    llm_precisions.append(llm_prec)\n",
    "    llm_recalls.append(llm_rec)\n",
    "\n",
    "    t2_precisions.append(t2_prec)\n",
    "    t2_recalls.append(t2_rec)\n",
    "\n",
    "# Compute means\n",
    "rebeca_prec_mean = torch.tensor(rebeca_precisions).mean()\n",
    "rebeca_rec_mean = torch.tensor(rebeca_recalls).mean()\n",
    "\n",
    "llm_prec_mean = torch.tensor(llm_precisions).mean()\n",
    "llm_rec_mean = torch.tensor(llm_recalls).mean()\n",
    "\n",
    "t2_prec_mean = torch.tensor(t2_precisions).mean()\n",
    "t2_rec_mean = torch.tensor(t2_recalls).mean()\n",
    "\n",
    "# Display\n",
    "print(\"== AVERAGE PRECISIONS ==\")\n",
    "print(\"REBECA:\", rebeca_prec_mean.item())\n",
    "print(\"LLM   :\", llm_prec_mean.item())\n",
    "print(\"T2    :\", t2_prec_mean.item())\n",
    "\n",
    "print(\"== AVERAGE RECALLS ==\")\n",
    "print(\"REBECA:\", rebeca_rec_mean.item())\n",
    "print(\"LLM   :\", llm_rec_mean.item())\n",
    "print(\"T2    :\", t2_rec_mean.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.125, 1.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rebeca_embeddings': tensor([[-0.1105,  0.0246,  0.2082,  ...,  0.2820,  0.7341,  1.0000],\n",
       "         [ 0.6866, -0.0304, -0.1738,  ...,  0.4561, -0.4463,  0.3548],\n",
       "         [-0.2670,  0.4138, -0.4932,  ..., -0.2317,  0.3979, -0.3279],\n",
       "         ...,\n",
       "         [-0.3196,  0.3060,  0.3436,  ..., -0.7037, -0.2401, -0.0504],\n",
       "         [ 0.3215,  0.6854,  0.8581,  ...,  0.2277, -0.4329, -0.1015],\n",
       "         [-0.3521,  0.4845, -0.3631,  ...,  0.5872, -0.9760, -0.0237]]),\n",
       " 'llm_embeddings': tensor([[-1.0374,  1.3192,  0.0919,  ..., -0.1950, -0.9320,  0.9253],\n",
       "         [-0.0185, -0.0692, -0.5141,  ..., -0.7484, -0.0904, -0.8282],\n",
       "         [-0.0450,  0.4084, -0.0857,  ..., -1.2065,  0.0538, -0.7014],\n",
       "         ...,\n",
       "         [-0.2528,  0.8283, -0.5241,  ..., -0.1881, -0.2135, -0.7562],\n",
       "         [-0.3547,  0.2377, -0.2352,  ..., -0.7045, -0.0153, -0.5422],\n",
       "         [-0.5085,  0.2119,  0.0417,  ..., -0.7597, -0.9715, -0.3863]]),\n",
       " 'T2': tensor([[-0.2513,  0.1295, -0.3730,  ..., -0.9222, -0.4242, -0.1011],\n",
       "         [ 0.1578,  1.0238, -0.2218,  ...,  0.7238,  0.7713, -0.3102],\n",
       "         [ 0.2556, -0.1344,  0.0700,  ..., -0.3193, -0.3334,  0.4470],\n",
       "         ...,\n",
       "         [ 0.1855,  0.2280, -0.8049,  ..., -0.8099, -0.0117,  0.1889],\n",
       "         [-0.3200, -0.2840, -0.2359,  ..., -0.6677, -0.3556, -0.9452],\n",
       "         [-0.8630, -0.7338,  0.4586,  ..., -0.3214, -0.5842,  0.0152]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = torch.load(\"../data/flickr/processed/ip-adapters/SD15/sd15_image_embeddings.pt\", weights_only=True)\n",
    "filtered_ratings_df = pd.read_csv(\"../data/flickr/processed/filtered_ratings_df_usrthrs_100.csv\")\n",
    "#expanded_features = map_embeddings_to_ratings(image_features, filtered_ratings_df)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 177278\n",
      "Validation set size: 928\n",
      "Evaluation set size: 933\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = split_recommender_data(\n",
    "    ratings_df=filtered_ratings_df,\n",
    "    val_spu=10,\n",
    "    test_spu=10,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "liked_df = test_df[test_df[\"score\"]>=4]\n",
    "test_df.to_csv(\"../data/flickr/evaluation/ground_truth/usrthrs_100/test_df.csv\")\n",
    "liked_df.to_csv(\"../data/flickr/evaluation/ground_truth/usrthrs_100/liked_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>worker</th>\n",
       "      <th>imagePair</th>\n",
       "      <th>score</th>\n",
       "      <th>old_worker_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>worker_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>863</td>\n",
       "      <td>910</td>\n",
       "      <td>A14W0IW2KGR80K</td>\n",
       "      <td>farm8_7412_9119468631_6db230d056.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4783</td>\n",
       "      <td>5803</td>\n",
       "      <td>A219DFCY05R0WJ</td>\n",
       "      <td>farm9_8232_8402674921_616367cd0f.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5466</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4911</td>\n",
       "      <td>5939</td>\n",
       "      <td>A219DFCY05R0WJ</td>\n",
       "      <td>farm8_7048_6935541584_449a69270b.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5601</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5502</td>\n",
       "      <td>6560</td>\n",
       "      <td>A219DFCY05R0WJ</td>\n",
       "      <td>farm1_109_316029216_7f07904970.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5575</td>\n",
       "      <td>6636</td>\n",
       "      <td>A219DFCY05R0WJ</td>\n",
       "      <td>farm9_8495_8269813182_c294c36f45.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>178695</td>\n",
       "      <td>202524</td>\n",
       "      <td>A37CZZH18KQ2V2</td>\n",
       "      <td>farm8_7353_26951283693_38b1fb2a8c_b.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>208</td>\n",
       "      <td>18952</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>178941</td>\n",
       "      <td>202780</td>\n",
       "      <td>A37CZZH18KQ2V2</td>\n",
       "      <td>farm9_8287_7647087844_5a1c16694a.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>208</td>\n",
       "      <td>37452</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>178979</td>\n",
       "      <td>202820</td>\n",
       "      <td>A37CZZH18KQ2V2</td>\n",
       "      <td>farm2_1087_709243977_b9afa0f5d3.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>208</td>\n",
       "      <td>29047</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>179055</td>\n",
       "      <td>202900</td>\n",
       "      <td>A37CZZH18KQ2V2</td>\n",
       "      <td>farm1_766_23195362051_fd5ce4d44a_b.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>208</td>\n",
       "      <td>38296</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>179064</td>\n",
       "      <td>202909</td>\n",
       "      <td>A37CZZH18KQ2V2</td>\n",
       "      <td>farm3_2770_4112006396_f124de33fe.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>208</td>\n",
       "      <td>38305</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     original_index  Unnamed: 0          worker  \\\n",
       "2               863         910  A14W0IW2KGR80K   \n",
       "10             4783        5803  A219DFCY05R0WJ   \n",
       "11             4911        5939  A219DFCY05R0WJ   \n",
       "12             5502        6560  A219DFCY05R0WJ   \n",
       "13             5575        6636  A219DFCY05R0WJ   \n",
       "..              ...         ...             ...   \n",
       "923          178695      202524  A37CZZH18KQ2V2   \n",
       "927          178941      202780  A37CZZH18KQ2V2   \n",
       "929          178979      202820  A37CZZH18KQ2V2   \n",
       "931          179055      202900  A37CZZH18KQ2V2   \n",
       "932          179064      202909  A37CZZH18KQ2V2   \n",
       "\n",
       "                                   imagePair  score  old_worker_id  image_id  \\\n",
       "2       farm8_7412_9119468631_6db230d056.jpg      5              0       909   \n",
       "10      farm9_8232_8402674921_616367cd0f.jpg      5              6      5466   \n",
       "11      farm8_7048_6935541584_449a69270b.jpg      4              6      5601   \n",
       "12        farm1_109_316029216_7f07904970.jpg      4              6      6108   \n",
       "13      farm9_8495_8269813182_c294c36f45.jpg      5              6      6183   \n",
       "..                                       ...    ...            ...       ...   \n",
       "923  farm8_7353_26951283693_38b1fb2a8c_b.jpg      5            208     18952   \n",
       "927     farm9_8287_7647087844_5a1c16694a.jpg      5            208     37452   \n",
       "929      farm2_1087_709243977_b9afa0f5d3.jpg      5            208     29047   \n",
       "931   farm1_766_23195362051_fd5ce4d44a_b.jpg      5            208     38296   \n",
       "932     farm3_2770_4112006396_f124de33fe.jpg      5            208     38305   \n",
       "\n",
       "     worker_id  \n",
       "2            0  \n",
       "10           1  \n",
       "11           1  \n",
       "12           1  \n",
       "13           1  \n",
       "..         ...  \n",
       "923         93  \n",
       "927         93  \n",
       "929         93  \n",
       "931         93  \n",
       "932         93  \n",
       "\n",
       "[352 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_eval_images(\n",
    "    paths_iter=liked_df[\"imagePair\"],\n",
    "    src_dir=\"../data/flickr/raw/40K\",\n",
    "    dst_dir=\"../data/flickr/evaluation/ground_truth/usrthrs_100/liked/images/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_eval_images_per_user(\n",
    "    liked_df=liked_df,\n",
    "    src_dir=\"../data/flickr/raw/40K\",\n",
    "    dst_base_dir=\"../data/flickr/evaluation/ground_truth/usrthrs_100/liked_per_user/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(liked_df[\"worker_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\.cache\\huggingface\\hub\\models--runwayml--stable-diffusion-v1-5\\snapshots\\451f4fe16113bff5a5d2269ed5ad43b0592e9a14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a96e7dc265b4bb48db7fb023b2c2b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"                                                                                                                                                                                                                \n",
    "pipe = StableDiffusionPipelineAdapterEmbeddings.from_pretrained(model_id).to(\"cuda\")\n",
    "pipe.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")     \n",
    "pipe.safety_checker = None\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:16<00:00, 20.66it/s]\n"
     ]
    }
   ],
   "source": [
    "sd15_liked_embeddings = compute_embeddings(\n",
    "    diffusion_pipe=pipe,\n",
    "    image_paths=\"../data/flickr/evaluation/ground_truth/usrthrs_100/liked/images/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sd15_liked_embeddings, \"../data/flickr/evaluation/ground_truth/usrthrs_100/liked/embeddings/sd15_embeddings.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_DIRS = \"../data/flickr/evaluation/ground_truth/usrthrs_100/liked_per_user\"\n",
    "\n",
    "for user in os.listdir(USER_DIRS):\n",
    "    user_id = user.split(\"_\")[1]\n",
    "    user_dir = os.path.join(USER_DIRS, f\"user_{user_id}\")\n",
    "    images_dir = os.path.join(user_dir, \"images\")\n",
    "    sdxl_ipadapter_embs = []\n",
    "    with torch.no_grad():\n",
    "        for path in os.listdir(images_dir):\n",
    "                #path = \"../data/raw/FLICKR-AES-001/40K/\" + unq_img_path\n",
    "            impath = os.path.join(images_dir, path)\n",
    "            pil_image = load_image(impath)\n",
    "            image_emb = pipe.encode_image(pil_image, device=\"cuda\", num_images_per_prompt=1)[0].squeeze()\n",
    "            sdxl_ipadapter_embs.append(image_emb.cpu())\n",
    "        sdxl_ipadapter_embs_tensor = torch.stack(sdxl_ipadapter_embs)\n",
    "        os.makedirs(os.path.join(user_dir, \"embeddings\"), exist_ok=True)\n",
    "        torch.save(sdxl_ipadapter_embs_tensor, f\"{images_dir}/../embeddings/sd15_embeddings.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\.cache\\huggingface\\hub\\models--stabilityai--stable-diffusion-xl-base-1.0\\snapshots\\462165984030d82259a11f4367a4eed129e94a7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487eb39aa0c14d3cb2744f7e92b08c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = StableDiffusionXLPipelineAdapterEmbeddings.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "pipe.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [00:16<00:00, 21.01it/s]\n"
     ]
    }
   ],
   "source": [
    "sdxl_liked_embeddings = compute_embeddings(\n",
    "    diffusion_pipe=pipe,\n",
    "    image_paths=\"../data/flickr/evaluation/ground_truth/usrthrs_100/liked/images/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sdxl_liked_embeddings, \"../data/flickr/evaluation/ground_truth/usrthrs_100/liked/embeddings/sdxl_embeddings.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "USER_DIRS = \"../data/flickr/evaluation/ground_truth/usrthrs_100/liked_per_user\"\n",
    "\n",
    "for user in os.listdir(USER_DIRS):\n",
    "    user_id = user.split(\"_\")[1]\n",
    "    user_dir = os.path.join(USER_DIRS, f\"user_{user_id}\")\n",
    "    images_dir = os.path.join(user_dir, \"images\")\n",
    "    sdxl_ipadapter_embs = []\n",
    "    with torch.no_grad():\n",
    "        for path in os.listdir(images_dir):\n",
    "                #path = \"../data/raw/FLICKR-AES-001/40K/\" + unq_img_path\n",
    "            impath = os.path.join(images_dir, path)\n",
    "            pil_image = load_image(impath)\n",
    "            image_emb = pipe.encode_image(pil_image, device=\"cuda\", num_images_per_prompt=1)[0].squeeze()\n",
    "            sdxl_ipadapter_embs.append(image_emb.cpu())\n",
    "        sdxl_ipadapter_embs_tensor = torch.stack(sdxl_ipadapter_embs)\n",
    "        os.makedirs(os.path.join(user_dir, \"embeddings\"))\n",
    "        torch.save(sdxl_ipadapter_embs_tensor, f\"{images_dir}/../embeddings/sdxl_embeddings.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5010/5010 [04:42<00:00, 17.73it/s]\n"
     ]
    }
   ],
   "source": [
    "sd15_baseline_t0_embeddings = compute_embeddings(\n",
    "    diffusion_pipe=pipe,\n",
    "    image_paths=\"../data/flickr/evaluation/baselines/t0_prompt/images/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sd15_baseline_t0_embeddings, \"../data/flickr/evaluation/baselines/t0_prompt/embeddings/embeddings.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5009/5009 [04:21<00:00, 19.18it/s]\n"
     ]
    }
   ],
   "source": [
    "sd15_baseline_t1_embeddings = compute_embeddings(\n",
    "    diffusion_pipe=pipe,\n",
    "    image_paths=\"../data/flickr/evaluation/baselines/t1_prompt/images/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sd15_baseline_t1_embeddings, \"../data/flickr/evaluation/baselines/t1_prompt/embeddings/embeddings.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hohlx\\anaconda3\\envs\\regenesys-env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\hohlx\\AppData\\Local\\Temp\\ipykernel_23408\\15620472.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  diffusion_prior_model.load_state_dict(torch.load(savepath))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerEmbeddingDiffusionModelv2(\n",
       "  (user_embedding): Embedding(210, 128)\n",
       "  (score_embedding): Embedding(2, 128)\n",
       "  (time_embedding): SinusoidalEmbedding()\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_fc): Linear(in_features=128, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_prior_model = TransformerEmbeddingDiffusionModelv2(\n",
    "    img_embed_dim=1024,\n",
    "    num_users=210,    # So user embedding covers your entire user set\n",
    "    n_heads=16,\n",
    "    num_tokens=8,\n",
    "    num_user_tokens=1,\n",
    "    num_layers=8,\n",
    "    dim_feedforward=1024\n",
    ").to(device)\n",
    "\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=6000)\n",
    "\n",
    "savepath = f\"../weights/grid_search_3/sd15_nl8_heads32_dim_feedforward1024_lr0.0001_it8_ut1_adamw_reduce_on_plateau_bs64_nslinear_spu80_timesteps6000_objz0-pred.pth\"\n",
    "\n",
    "diffusion_prior_model.load_state_dict(torch.load(savepath))\n",
    "diffusion_prior_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import ModelEvaluator\n",
    "from Datasets import FlatImageDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID Score (synthetic test): -0.11057186126708984\n"
     ]
    }
   ],
   "source": [
    "from torcheval.metrics import FrechetInceptionDistance\n",
    "import torch\n",
    "\n",
    "# Initialize FID metric\n",
    "fid_metric = FrechetInceptionDistance()\n",
    "\n",
    "# Create synthetic identical datasets\n",
    "real_images = torch.rand(20, 3, 299, 299)  # Random data\n",
    "fake_images = real_images.clone()           # Exact copy of real data\n",
    "\n",
    "# Update metric\n",
    "fid_metric.update(real_images, is_real=True)\n",
    "fid_metric.update(fake_images, is_real=False)\n",
    "\n",
    "# Compute FID\n",
    "score = fid_metric.compute()\n",
    "print(f\"FID Score (synthetic test): {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = ModelEvaluator(\n",
    "    diffusion_prior_model=diffusion_prior_model,\n",
    "    scheduler=noise_scheduler,\n",
    "    model_dir=\"../data/flickr/evaluation/diffusion_priors/experiment_2/model_2/\",\n",
    "    original_img_dir=\"../data/flickr/evaluation/diffusion_priors/experiment_2/original_eval_data/images/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [01:56<00:00,  2.84s/it]\n",
      "100%|██████████| 33/33 [01:30<00:00,  2.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(76.5065)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me.eval_fid(\n",
    "    path_original=\"../data/flickr/evaluation/diffusion_priors/experiment_2/original_eval_data/images/\",\n",
    "    path_generated=\"../data/flickr/evaluation/diffusion_priors/experiment_2/model_2/images/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings for 2597 images from ../data/flickr/evaluation/diffusion_priors/experiment_2/original_eval_data/images/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [01:32<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings for 2100 images from ../data/flickr/evaluation/diffusion_priors/experiment_2/model_1/images/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [01:23<00:00,  1.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7269)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me.eval_cmmd(\n",
    "    path_original=\"../data/flickr/evaluation/diffusion_priors/experiment_2/original_eval_data/images/\",\n",
    "    path_generated=\"../data/flickr/evaluation/diffusion_priors/experiment_2/model_1/images/\", \n",
    "    #path_original_emb=\"../data/flickr/evaluation/diffusion_priors/experiment_2/original_eval_data/embeddings/cmmd_embeddings.pth\",\n",
    "    #path_generated_emb=\"../data/flickr/evaluation/diffusion_priors/experiment_2/model_2/embeddings/cmmd_embeddings.pth\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recgensys-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
