{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom SDXL Diffusion Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Datasets import RecommenderUserSampler, EmbeddingsDataset\n",
    "from grid_search import run_grid_search\n",
    "from prior_models import TransformerEmbeddingDiffusionModelv2\n",
    "from train_priors import train_diffusion_prior\n",
    "from utils import map_embeddings_to_ratings, split_recommender_data, set_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the data in its corresponding (sub)directory and map image embeddings to observations.\n",
    "The data in ratings.csv will constitute our observations, and for our purposes, it will \n",
    "consist of the triplets $(U_i, S_j, I_k)$, where $U_i$ corresponds user $i$, $S_j$ encodes wheter user likes $(\\text{ score}\\geq 4)$ or dislikes the image $(\\text{ score}< 4)$ and $I_k$ is the $k$-th image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = torch.load(\"../data/flickr/processed/ip-adapters/SDXL/sdxl_image_embeddings.pt\", weights_only=True)\n",
    "ratings_df = pd.read_csv(\"../data/flickr/processed/ratings.csv\")\n",
    "expanded_features = map_embeddings_to_ratings(image_features, ratings_df)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User loss: 22\n",
      "Data loss: 0.00664051178005054%\n"
     ]
    }
   ],
   "source": [
    "liked_counts = (\n",
    "    ratings_df[ratings_df[\"score\"] >= 4]\n",
    "    .groupby(\"worker_id\")[\"score\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"liked_count\")\n",
    ")\n",
    "valid_users = liked_counts[liked_counts[\"liked_count\"] >= 20][\"worker_id\"].unique()\n",
    "valid_worker_id = liked_counts[liked_counts[\"liked_count\"] >= 20][\"worker_id\"].unique()\n",
    "filtered_ratings_df = ratings_df[ratings_df[\"worker_id\"].isin(valid_users)].copy()\n",
    "print(f\"User loss: {210-len(valid_users)}\")\n",
    "print(f\"Data loss: {1 - filtered_ratings_df.shape[0]/ratings_df.shape[0]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_mapping = {old_id: new_id for new_id, old_id in enumerate(valid_worker_id)}\n",
    "filtered_ratings_df.rename(columns={\"worker_id\": \"old_worker_id\"}, inplace=True)\n",
    "filtered_ratings_df[\"worker_id\"] = filtered_ratings_df[\"old_worker_id\"].map(worker_mapping)\n",
    "#filtered_ratings_df = filtered_ratings_df.reset_index(drop=True)\n",
    "worker_mapping_df = pd.DataFrame(list(worker_mapping.items()), columns=[\"old_worker_id\", \"worker_id\"])\n",
    "worker_mapping_df.to_csv(\"../data/flickr/processed/worker_id_mapping.csv\", index=False)\n",
    "filtered_ratings_df.to_csv(\"../data/flickr/processed/filtered_ratings_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 188273\n",
      "Validation set size: 1823\n",
      "Evaluation set size: 1829\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = split_recommender_data(\n",
    "    ratings_df=filtered_ratings_df,\n",
    "    val_spu=10,\n",
    "    test_spu=10,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"../data/flickr/processed/train/train.csv\", index=False)\n",
    "val_df.to_csv(\"../data/flickr/processed/train/validation.csv\", index=False)\n",
    "test_df.to_csv(\"../data/flickr/processed/test/test.csv\", index=False)\n",
    "\n",
    "train_ie = expanded_features[train_df.original_index]\n",
    "val_ie = expanded_features[val_df.original_index]\n",
    "test_ie = expanded_features[test_df.original_index]\n",
    "\n",
    "torch.save(train_ie, \"../data/flickr/processed/train/train_ie.pth\")\n",
    "torch.save(val_ie, \"../data/flickr/processed/train/val_ie.pth\")\n",
    "torch.save(test_ie, \"../data/flickr/processed/test/test_ie.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingsDataset(\n",
    "    train_df,\n",
    "    image_embeddings=expanded_features[train_df.original_index]\n",
    ")\n",
    "\n",
    "val_dataset = EmbeddingsDataset(\n",
    "    val_df,\n",
    "    image_embeddings=expanded_features[val_df.original_index]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "diffusion_prior_model = TransformerEmbeddingDiffusionModelv2(\n",
    "    img_embed_dim=1280,\n",
    "    num_users=188,    # So user embedding covers your entire user set\n",
    "    n_heads=16,\n",
    "    num_tokens=1,\n",
    "    num_user_tokens=4,\n",
    "    num_layers=8,\n",
    "    dim_feedforward=2048,\n",
    "    whether_use_user_embeddings=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 97092864\n",
      "Trainable parameters: 97092864\n"
     ]
    }
   ],
   "source": [
    "set_seeds(0)\n",
    "batch_size = 64\n",
    "samples_per_user = 50\n",
    "learning_rate = 1e-4\n",
    "unique_users = filtered_ratings_df[\"worker_id\"].unique()\n",
    "train_user_sampler = RecommenderUserSampler(train_df, num_users=len(unique_users), samples_per_user=samples_per_user)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_user_sampler, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "diffusion_optimizer = torch.optim.AdamW(diffusion_prior_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=6000)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(diffusion_optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "total_params = sum(p.numel() for p in diffusion_prior_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in diffusion_prior_model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "savepath = f\"../data/flickr/evaluation/diffusion_priors/sdxl_ied1024_nu188_nh16_nit1_nut4_nl8_dff1024_uetrue.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001, Time Elapsed: 6.15s, Train Loss: 1.4693, Val Loss: 1.2755, Grad Norm: 5.3657\n",
      "Epoch 2/2001, Time Elapsed: 12.23s, Train Loss: 1.1523, Val Loss: 1.0927, Grad Norm: 3.1856\n",
      "Epoch 3/2001, Time Elapsed: 18.29s, Train Loss: 0.9786, Val Loss: 0.9156, Grad Norm: 2.5721\n",
      "Epoch 4/2001, Time Elapsed: 24.34s, Train Loss: 0.8328, Val Loss: 0.7829, Grad Norm: 2.3149\n",
      "Epoch 5/2001, Time Elapsed: 30.41s, Train Loss: 0.7254, Val Loss: 0.6746, Grad Norm: 2.1344\n",
      "Epoch 6/2001, Time Elapsed: 36.35s, Train Loss: 0.6461, Val Loss: 0.5914, Grad Norm: 2.0181\n",
      "Epoch 7/2001, Time Elapsed: 42.20s, Train Loss: 0.5841, Val Loss: 0.5280, Grad Norm: 1.9133\n",
      "Epoch 8/2001, Time Elapsed: 48.01s, Train Loss: 0.5336, Val Loss: 0.4853, Grad Norm: 1.8305\n",
      "Epoch 9/2001, Time Elapsed: 53.88s, Train Loss: 0.4940, Val Loss: 0.4433, Grad Norm: 1.7693\n",
      "Epoch 10/2001, Time Elapsed: 59.85s, Train Loss: 0.4577, Val Loss: 0.4084, Grad Norm: 1.7067\n",
      "Epoch 11/2001, Time Elapsed: 65.75s, Train Loss: 0.4280, Val Loss: 0.3674, Grad Norm: 1.6629\n",
      "Epoch 12/2001, Time Elapsed: 71.84s, Train Loss: 0.4013, Val Loss: 0.3433, Grad Norm: 1.6180\n",
      "Epoch 13/2001, Time Elapsed: 77.97s, Train Loss: 0.3772, Val Loss: 0.3194, Grad Norm: 1.5739\n",
      "Epoch 14/2001, Time Elapsed: 84.09s, Train Loss: 0.3617, Val Loss: 0.3028, Grad Norm: 1.5533\n",
      "Epoch 15/2001, Time Elapsed: 90.26s, Train Loss: 0.3384, Val Loss: 0.2917, Grad Norm: 1.5117\n",
      "Epoch 16/2001, Time Elapsed: 96.26s, Train Loss: 0.3237, Val Loss: 0.2668, Grad Norm: 1.4769\n",
      "Epoch 17/2001, Time Elapsed: 102.21s, Train Loss: 0.3087, Val Loss: 0.2554, Grad Norm: 1.4453\n",
      "Epoch 18/2001, Time Elapsed: 108.13s, Train Loss: 0.2905, Val Loss: 0.2408, Grad Norm: 1.4004\n",
      "Epoch 19/2001, Time Elapsed: 114.21s, Train Loss: 0.2805, Val Loss: 0.2307, Grad Norm: 1.3780\n",
      "Epoch 20/2001, Time Elapsed: 120.24s, Train Loss: 0.2719, Val Loss: 0.2192, Grad Norm: 1.3520\n",
      "Epoch 21/2001, Time Elapsed: 126.34s, Train Loss: 0.2565, Val Loss: 0.2080, Grad Norm: 1.3149\n",
      "Epoch 22/2001, Time Elapsed: 132.39s, Train Loss: 0.2504, Val Loss: 0.2083, Grad Norm: 1.2935\n",
      "Epoch 23/2001, Time Elapsed: 138.19s, Train Loss: 0.2405, Val Loss: 0.1902, Grad Norm: 1.2691\n",
      "Epoch 24/2001, Time Elapsed: 144.44s, Train Loss: 0.2346, Val Loss: 0.1945, Grad Norm: 1.2513\n",
      "Epoch 25/2001, Time Elapsed: 150.39s, Train Loss: 0.2232, Val Loss: 0.1765, Grad Norm: 1.2200\n",
      "Epoch 26/2001, Time Elapsed: 156.54s, Train Loss: 0.2170, Val Loss: 0.1790, Grad Norm: 1.2061\n",
      "Epoch 27/2001, Time Elapsed: 162.34s, Train Loss: 0.2151, Val Loss: 0.1725, Grad Norm: 1.1930\n",
      "Epoch 28/2001, Time Elapsed: 168.44s, Train Loss: 0.2097, Val Loss: 0.1647, Grad Norm: 1.1766\n",
      "Epoch 29/2001, Time Elapsed: 174.51s, Train Loss: 0.1990, Val Loss: 0.1632, Grad Norm: 1.1465\n",
      "Epoch 30/2001, Time Elapsed: 180.61s, Train Loss: 0.1955, Val Loss: 0.1574, Grad Norm: 1.1362\n",
      "Epoch 31/2001, Time Elapsed: 186.54s, Train Loss: 0.1947, Val Loss: 0.1577, Grad Norm: 1.1255\n",
      "Epoch 32/2001, Time Elapsed: 192.19s, Train Loss: 0.1857, Val Loss: 0.1479, Grad Norm: 1.1032\n",
      "Epoch 33/2001, Time Elapsed: 198.25s, Train Loss: 0.1832, Val Loss: 0.1385, Grad Norm: 1.0876\n",
      "Epoch 34/2001, Time Elapsed: 204.21s, Train Loss: 0.1812, Val Loss: 0.1492, Grad Norm: 1.0814\n",
      "Epoch 35/2001, Time Elapsed: 209.97s, Train Loss: 0.1733, Val Loss: 0.1337, Grad Norm: 1.0577\n",
      "Epoch 36/2001, Time Elapsed: 216.10s, Train Loss: 0.1719, Val Loss: 0.1397, Grad Norm: 1.0476\n",
      "Epoch 37/2001, Time Elapsed: 221.90s, Train Loss: 0.1694, Val Loss: 0.1300, Grad Norm: 1.0397\n",
      "Epoch 38/2001, Time Elapsed: 228.00s, Train Loss: 0.1666, Val Loss: 0.1328, Grad Norm: 1.0265\n",
      "Epoch 39/2001, Time Elapsed: 233.80s, Train Loss: 0.1565, Val Loss: 0.1222, Grad Norm: 0.9981\n",
      "Epoch 40/2001, Time Elapsed: 239.92s, Train Loss: 0.1577, Val Loss: 0.1233, Grad Norm: 0.9931\n",
      "Epoch 41/2001, Time Elapsed: 245.75s, Train Loss: 0.1619, Val Loss: 0.1325, Grad Norm: 1.0003\n",
      "Epoch 42/2001, Time Elapsed: 251.57s, Train Loss: 0.1582, Val Loss: 0.1213, Grad Norm: 0.9869\n",
      "Epoch 43/2001, Time Elapsed: 257.65s, Train Loss: 0.1537, Val Loss: 0.1183, Grad Norm: 0.9704\n",
      "Epoch 44/2001, Time Elapsed: 263.75s, Train Loss: 0.1501, Val Loss: 0.1302, Grad Norm: 0.9577\n",
      "Epoch 45/2001, Time Elapsed: 269.58s, Train Loss: 0.1535, Val Loss: 0.1273, Grad Norm: 0.9597\n",
      "Epoch 46/2001, Time Elapsed: 275.38s, Train Loss: 0.1442, Val Loss: 0.1096, Grad Norm: 0.9317\n",
      "Epoch 47/2001, Time Elapsed: 281.50s, Train Loss: 0.1440, Val Loss: 0.1187, Grad Norm: 0.9277\n",
      "Epoch 48/2001, Time Elapsed: 287.32s, Train Loss: 0.1458, Val Loss: 0.1267, Grad Norm: 0.9281\n",
      "Epoch 49/2001, Time Elapsed: 293.15s, Train Loss: 0.1401, Val Loss: 0.1109, Grad Norm: 0.9107\n",
      "Epoch 50/2001, Time Elapsed: 298.96s, Train Loss: 0.1382, Val Loss: 0.1129, Grad Norm: 0.9013\n",
      "Epoch 51/2001, Time Elapsed: 304.78s, Train Loss: 0.1395, Val Loss: 0.1133, Grad Norm: 0.9007\n",
      "Epoch 52/2001, Time Elapsed: 310.61s, Train Loss: 0.1370, Val Loss: 0.1048, Grad Norm: 0.8897\n",
      "Epoch 53/2001, Time Elapsed: 316.73s, Train Loss: 0.1330, Val Loss: 0.1100, Grad Norm: 0.8749\n",
      "Epoch 54/2001, Time Elapsed: 322.59s, Train Loss: 0.1326, Val Loss: 0.1031, Grad Norm: 0.8729\n",
      "Epoch 55/2001, Time Elapsed: 328.78s, Train Loss: 0.1325, Val Loss: 0.1164, Grad Norm: 0.8685\n",
      "Epoch 56/2001, Time Elapsed: 334.63s, Train Loss: 0.1312, Val Loss: 0.0989, Grad Norm: 0.8602\n",
      "Epoch 57/2001, Time Elapsed: 340.77s, Train Loss: 0.1329, Val Loss: 0.1063, Grad Norm: 0.8616\n",
      "Epoch 58/2001, Time Elapsed: 346.58s, Train Loss: 0.1308, Val Loss: 0.0976, Grad Norm: 0.8529\n",
      "Epoch 59/2001, Time Elapsed: 352.66s, Train Loss: 0.1290, Val Loss: 0.1059, Grad Norm: 0.8478\n",
      "Epoch 60/2001, Time Elapsed: 358.48s, Train Loss: 0.1275, Val Loss: 0.0978, Grad Norm: 0.8389\n",
      "Epoch 61/2001, Time Elapsed: 364.28s, Train Loss: 0.1273, Val Loss: 0.0924, Grad Norm: 0.8352\n",
      "Epoch 62/2001, Time Elapsed: 370.35s, Train Loss: 0.1237, Val Loss: 0.1084, Grad Norm: 0.8229\n",
      "Epoch 63/2001, Time Elapsed: 376.18s, Train Loss: 0.1235, Val Loss: 0.0971, Grad Norm: 0.8200\n",
      "Epoch 64/2001, Time Elapsed: 382.00s, Train Loss: 0.1244, Val Loss: 0.1013, Grad Norm: 0.8222\n",
      "Epoch 65/2001, Time Elapsed: 387.85s, Train Loss: 0.1190, Val Loss: 0.1023, Grad Norm: 0.8018\n",
      "Epoch 66/2001, Time Elapsed: 393.73s, Train Loss: 0.1200, Val Loss: 0.1011, Grad Norm: 0.8039\n",
      "Epoch 67/2001, Time Elapsed: 399.56s, Train Loss: 0.1186, Val Loss: 0.1067, Grad Norm: 0.7978\n",
      "Epoch 68/2001, Time Elapsed: 405.31s, Train Loss: 0.1207, Val Loss: 0.0996, Grad Norm: 0.7668\n",
      "Epoch 69/2001, Time Elapsed: 411.06s, Train Loss: 0.1129, Val Loss: 0.0855, Grad Norm: 0.7424\n",
      "Epoch 70/2001, Time Elapsed: 417.10s, Train Loss: 0.1112, Val Loss: 0.0957, Grad Norm: 0.7378\n",
      "Epoch 71/2001, Time Elapsed: 422.85s, Train Loss: 0.1098, Val Loss: 0.0897, Grad Norm: 0.7361\n",
      "Epoch 72/2001, Time Elapsed: 428.66s, Train Loss: 0.1098, Val Loss: 0.0900, Grad Norm: 0.7352\n",
      "Epoch 73/2001, Time Elapsed: 434.51s, Train Loss: 0.1087, Val Loss: 0.0871, Grad Norm: 0.7329\n",
      "Epoch 74/2001, Time Elapsed: 440.41s, Train Loss: 0.1098, Val Loss: 0.0930, Grad Norm: 0.7367\n",
      "Epoch 75/2001, Time Elapsed: 446.29s, Train Loss: 0.1059, Val Loss: 0.0868, Grad Norm: 0.7260\n",
      "Epoch 76/2001, Time Elapsed: 452.18s, Train Loss: 0.1061, Val Loss: 0.0908, Grad Norm: 0.7048\n",
      "Epoch 77/2001, Time Elapsed: 458.08s, Train Loss: 0.1094, Val Loss: 0.0896, Grad Norm: 0.7170\n",
      "Epoch 78/2001, Time Elapsed: 463.89s, Train Loss: 0.1034, Val Loss: 0.0862, Grad Norm: 0.6936\n",
      "Epoch 79/2001, Time Elapsed: 469.73s, Train Loss: 0.1060, Val Loss: 0.0834, Grad Norm: 0.7052\n",
      "Epoch 80/2001, Time Elapsed: 475.84s, Train Loss: 0.1065, Val Loss: 0.0917, Grad Norm: 0.7107\n",
      "Epoch 81/2001, Time Elapsed: 481.69s, Train Loss: 0.1032, Val Loss: 0.0925, Grad Norm: 0.6968\n",
      "Epoch 82/2001, Time Elapsed: 487.52s, Train Loss: 0.1056, Val Loss: 0.0823, Grad Norm: 0.7079\n",
      "Epoch 83/2001, Time Elapsed: 493.61s, Train Loss: 0.1017, Val Loss: 0.0929, Grad Norm: 0.6945\n",
      "Epoch 84/2001, Time Elapsed: 499.46s, Train Loss: 0.1038, Val Loss: 0.0856, Grad Norm: 0.7033\n",
      "Epoch 85/2001, Time Elapsed: 505.31s, Train Loss: 0.1012, Val Loss: 0.0819, Grad Norm: 0.6936\n",
      "Epoch 86/2001, Time Elapsed: 511.41s, Train Loss: 0.1059, Val Loss: 0.0844, Grad Norm: 0.7102\n",
      "Epoch 87/2001, Time Elapsed: 517.22s, Train Loss: 0.1090, Val Loss: 0.0843, Grad Norm: 0.7247\n",
      "Epoch 88/2001, Time Elapsed: 523.16s, Train Loss: 0.1013, Val Loss: 0.0853, Grad Norm: 0.6960\n",
      "Epoch 89/2001, Time Elapsed: 529.06s, Train Loss: 0.1046, Val Loss: 0.0839, Grad Norm: 0.7093\n",
      "Epoch 90/2001, Time Elapsed: 534.97s, Train Loss: 0.1043, Val Loss: 0.0818, Grad Norm: 0.7061\n",
      "Epoch 91/2001, Time Elapsed: 541.20s, Train Loss: 0.1036, Val Loss: 0.0852, Grad Norm: 0.7053\n",
      "Epoch 92/2001, Time Elapsed: 547.07s, Train Loss: 0.0984, Val Loss: 0.0857, Grad Norm: 0.6863\n",
      "Epoch 93/2001, Time Elapsed: 552.95s, Train Loss: 0.1054, Val Loss: 0.0814, Grad Norm: 0.7115\n",
      "Epoch 94/2001, Time Elapsed: 559.17s, Train Loss: 0.1013, Val Loss: 0.0821, Grad Norm: 0.6965\n",
      "Epoch 95/2001, Time Elapsed: 565.11s, Train Loss: 0.1000, Val Loss: 0.0781, Grad Norm: 0.6922\n",
      "Epoch 96/2001, Time Elapsed: 571.29s, Train Loss: 0.1008, Val Loss: 0.0819, Grad Norm: 0.6963\n",
      "Epoch 97/2001, Time Elapsed: 577.19s, Train Loss: 0.1000, Val Loss: 0.0764, Grad Norm: 0.6923\n",
      "Epoch 98/2001, Time Elapsed: 583.32s, Train Loss: 0.0988, Val Loss: 0.0797, Grad Norm: 0.6888\n",
      "Epoch 99/2001, Time Elapsed: 589.15s, Train Loss: 0.0996, Val Loss: 0.0775, Grad Norm: 0.6925\n",
      "Epoch 100/2001, Time Elapsed: 595.02s, Train Loss: 0.1006, Val Loss: 0.0810, Grad Norm: 0.6988\n",
      "Epoch 101/2001, Time Elapsed: 600.84s, Train Loss: 0.0968, Val Loss: 0.0742, Grad Norm: 0.6821\n",
      "Epoch 102/2001, Time Elapsed: 606.95s, Train Loss: 0.1001, Val Loss: 0.0805, Grad Norm: 0.6971\n",
      "Epoch 103/2001, Time Elapsed: 612.75s, Train Loss: 0.0981, Val Loss: 0.0734, Grad Norm: 0.6884\n",
      "Epoch 104/2001, Time Elapsed: 618.85s, Train Loss: 0.0994, Val Loss: 0.0826, Grad Norm: 0.6939\n",
      "Epoch 105/2001, Time Elapsed: 624.67s, Train Loss: 0.0984, Val Loss: 0.0776, Grad Norm: 0.6909\n",
      "Epoch 106/2001, Time Elapsed: 630.50s, Train Loss: 0.1009, Val Loss: 0.0834, Grad Norm: 0.6992\n",
      "Epoch 107/2001, Time Elapsed: 636.37s, Train Loss: 0.0993, Val Loss: 0.0878, Grad Norm: 0.6947\n",
      "Epoch 108/2001, Time Elapsed: 642.22s, Train Loss: 0.0961, Val Loss: 0.0810, Grad Norm: 0.6798\n",
      "Epoch 109/2001, Time Elapsed: 648.00s, Train Loss: 0.0985, Val Loss: 0.0781, Grad Norm: 0.6901\n",
      "Epoch 110/2001, Time Elapsed: 653.80s, Train Loss: 0.0969, Val Loss: 0.0779, Grad Norm: 0.6741\n",
      "Epoch 111/2001, Time Elapsed: 659.60s, Train Loss: 0.0980, Val Loss: 0.0776, Grad Norm: 0.6773\n",
      "Epoch 112/2001, Time Elapsed: 665.43s, Train Loss: 0.0992, Val Loss: 0.0822, Grad Norm: 0.6812\n",
      "Epoch 113/2001, Time Elapsed: 671.28s, Train Loss: 0.0961, Val Loss: 0.0760, Grad Norm: 0.6676\n",
      "Epoch 114/2001, Time Elapsed: 677.23s, Train Loss: 0.0993, Val Loss: 0.0809, Grad Norm: 0.6856\n",
      "Epoch 115/2001, Time Elapsed: 683.17s, Train Loss: 0.0971, Val Loss: 0.0763, Grad Norm: 0.6759\n",
      "Epoch 116/2001, Time Elapsed: 689.08s, Train Loss: 0.0991, Val Loss: 0.0775, Grad Norm: 0.6762\n",
      "Epoch 117/2001, Time Elapsed: 695.00s, Train Loss: 0.0983, Val Loss: 0.0693, Grad Norm: 0.6736\n",
      "Epoch 118/2001, Time Elapsed: 701.15s, Train Loss: 0.0941, Val Loss: 0.0771, Grad Norm: 0.6574\n",
      "Epoch 119/2001, Time Elapsed: 707.00s, Train Loss: 0.0939, Val Loss: 0.0775, Grad Norm: 0.6553\n",
      "Epoch 120/2001, Time Elapsed: 712.82s, Train Loss: 0.0958, Val Loss: 0.0773, Grad Norm: 0.6642\n",
      "Epoch 121/2001, Time Elapsed: 718.65s, Train Loss: 0.0931, Val Loss: 0.0751, Grad Norm: 0.6583\n",
      "Epoch 122/2001, Time Elapsed: 724.50s, Train Loss: 0.0964, Val Loss: 0.0762, Grad Norm: 0.6694\n",
      "Epoch 123/2001, Time Elapsed: 730.33s, Train Loss: 0.0954, Val Loss: 0.0824, Grad Norm: 0.6653\n",
      "Epoch 124/2001, Time Elapsed: 736.17s, Train Loss: 0.0972, Val Loss: 0.0758, Grad Norm: 0.6700\n",
      "Epoch 125/2001, Time Elapsed: 742.01s, Train Loss: 0.0954, Val Loss: 0.0802, Grad Norm: 0.6636\n",
      "Epoch 126/2001, Time Elapsed: 747.81s, Train Loss: 0.0957, Val Loss: 0.0688, Grad Norm: 0.6631\n",
      "Epoch 127/2001, Time Elapsed: 753.93s, Train Loss: 0.0957, Val Loss: 0.0793, Grad Norm: 0.6640\n",
      "Epoch 128/2001, Time Elapsed: 759.73s, Train Loss: 0.0961, Val Loss: 0.0773, Grad Norm: 0.6659\n",
      "Epoch 129/2001, Time Elapsed: 765.51s, Train Loss: 0.0960, Val Loss: 0.0728, Grad Norm: 0.6642\n",
      "Epoch 130/2001, Time Elapsed: 771.31s, Train Loss: 0.0961, Val Loss: 0.0783, Grad Norm: 0.6627\n",
      "Epoch 131/2001, Time Elapsed: 777.08s, Train Loss: 0.0959, Val Loss: 0.0812, Grad Norm: 0.6640\n",
      "Epoch 132/2001, Time Elapsed: 782.93s, Train Loss: 0.0947, Val Loss: 0.0712, Grad Norm: 0.6605\n",
      "Epoch 133/2001, Time Elapsed: 788.79s, Train Loss: 0.0949, Val Loss: 0.0804, Grad Norm: 0.6597\n",
      "Epoch 134/2001, Time Elapsed: 794.66s, Train Loss: 0.0917, Val Loss: 0.0771, Grad Norm: 0.6477\n",
      "Epoch 135/2001, Time Elapsed: 800.55s, Train Loss: 0.0931, Val Loss: 0.0755, Grad Norm: 0.6536\n",
      "Epoch 136/2001, Time Elapsed: 806.43s, Train Loss: 0.0936, Val Loss: 0.0788, Grad Norm: 0.6555\n",
      "Epoch 137/2001, Time Elapsed: 812.28s, Train Loss: 0.0950, Val Loss: 0.0809, Grad Norm: 0.6597\n",
      "Epoch 138/2001, Time Elapsed: 818.16s, Train Loss: 0.0953, Val Loss: 0.0829, Grad Norm: 0.6586\n",
      "Epoch 139/2001, Time Elapsed: 823.96s, Train Loss: 0.0970, Val Loss: 0.0789, Grad Norm: 0.6686\n",
      "Epoch 140/2001, Time Elapsed: 829.78s, Train Loss: 0.0965, Val Loss: 0.0782, Grad Norm: 0.6627\n",
      "Epoch 141/2001, Time Elapsed: 835.61s, Train Loss: 0.0917, Val Loss: 0.0749, Grad Norm: 0.6463\n",
      "Epoch 142/2001, Time Elapsed: 841.44s, Train Loss: 0.0949, Val Loss: 0.0735, Grad Norm: 0.6586\n",
      "Epoch 143/2001, Time Elapsed: 847.29s, Train Loss: 0.0969, Val Loss: 0.0775, Grad Norm: 0.6683\n",
      "Epoch 144/2001, Time Elapsed: 853.07s, Train Loss: 0.0927, Val Loss: 0.0796, Grad Norm: 0.6488\n",
      "Epoch 145/2001, Time Elapsed: 858.89s, Train Loss: 0.0974, Val Loss: 0.0771, Grad Norm: 0.6677\n",
      "Epoch 146/2001, Time Elapsed: 864.69s, Train Loss: 0.0946, Val Loss: 0.0769, Grad Norm: 0.6567\n",
      "Epoch 147/2001, Time Elapsed: 870.52s, Train Loss: 0.0941, Val Loss: 0.0743, Grad Norm: 0.6557\n",
      "Epoch 148/2001, Time Elapsed: 876.34s, Train Loss: 0.0942, Val Loss: 0.0750, Grad Norm: 0.6541\n",
      "Epoch 149/2001, Time Elapsed: 882.17s, Train Loss: 0.0951, Val Loss: 0.0772, Grad Norm: 0.6592\n",
      "Epoch 150/2001, Time Elapsed: 888.34s, Train Loss: 0.0954, Val Loss: 0.0771, Grad Norm: 0.6585\n",
      "Epoch 151/2001, Time Elapsed: 894.42s, Train Loss: 0.0936, Val Loss: 0.0809, Grad Norm: 0.6550\n",
      "Epoch 152/2001, Time Elapsed: 900.49s, Train Loss: 0.0956, Val Loss: 0.0787, Grad Norm: 0.6603\n",
      "Epoch 153/2001, Time Elapsed: 906.61s, Train Loss: 0.0961, Val Loss: 0.0768, Grad Norm: 0.6610\n",
      "Epoch 154/2001, Time Elapsed: 912.65s, Train Loss: 0.0923, Val Loss: 0.0802, Grad Norm: 0.6484\n",
      "Epoch 155/2001, Time Elapsed: 918.64s, Train Loss: 0.0951, Val Loss: 0.0826, Grad Norm: 0.6591\n",
      "Epoch 156/2001, Time Elapsed: 924.83s, Train Loss: 0.0944, Val Loss: 0.0798, Grad Norm: 0.6557\n",
      "Epoch 157/2001, Time Elapsed: 930.87s, Train Loss: 0.0958, Val Loss: 0.0719, Grad Norm: 0.6626\n",
      "Epoch 158/2001, Time Elapsed: 936.94s, Train Loss: 0.0921, Val Loss: 0.0726, Grad Norm: 0.6464\n",
      "Epoch 159/2001, Time Elapsed: 943.00s, Train Loss: 0.0946, Val Loss: 0.0789, Grad Norm: 0.6574\n",
      "Epoch 160/2001, Time Elapsed: 949.08s, Train Loss: 0.0930, Val Loss: 0.0734, Grad Norm: 0.6527\n",
      "Epoch 161/2001, Time Elapsed: 955.17s, Train Loss: 0.0947, Val Loss: 0.0762, Grad Norm: 0.6573\n",
      "Epoch 162/2001, Time Elapsed: 961.24s, Train Loss: 0.0924, Val Loss: 0.0748, Grad Norm: 0.6461\n",
      "Epoch 163/2001, Time Elapsed: 967.34s, Train Loss: 0.0937, Val Loss: 0.0805, Grad Norm: 0.6508\n",
      "Epoch 164/2001, Time Elapsed: 973.43s, Train Loss: 0.0959, Val Loss: 0.0779, Grad Norm: 0.6648\n",
      "Epoch 165/2001, Time Elapsed: 979.56s, Train Loss: 0.0960, Val Loss: 0.0762, Grad Norm: 0.6650\n",
      "Epoch 166/2001, Time Elapsed: 985.66s, Train Loss: 0.0953, Val Loss: 0.0713, Grad Norm: 0.6593\n",
      "Epoch 167/2001, Time Elapsed: 991.71s, Train Loss: 0.0950, Val Loss: 0.0770, Grad Norm: 0.6580\n",
      "Epoch 168/2001, Time Elapsed: 997.77s, Train Loss: 0.0945, Val Loss: 0.0719, Grad Norm: 0.6577\n",
      "Epoch 169/2001, Time Elapsed: 1003.81s, Train Loss: 0.0930, Val Loss: 0.0722, Grad Norm: 0.6499\n",
      "Epoch 170/2001, Time Elapsed: 1009.90s, Train Loss: 0.0960, Val Loss: 0.0696, Grad Norm: 0.6599\n",
      "Epoch 171/2001, Time Elapsed: 1016.01s, Train Loss: 0.0974, Val Loss: 0.0804, Grad Norm: 0.6653\n",
      "Epoch 172/2001, Time Elapsed: 1022.07s, Train Loss: 0.0946, Val Loss: 0.0745, Grad Norm: 0.6579\n",
      "Epoch 173/2001, Time Elapsed: 1028.15s, Train Loss: 0.0956, Val Loss: 0.0761, Grad Norm: 0.6597\n",
      "Epoch 174/2001, Time Elapsed: 1034.23s, Train Loss: 0.0931, Val Loss: 0.0809, Grad Norm: 0.6497\n",
      "Epoch 175/2001, Time Elapsed: 1040.28s, Train Loss: 0.0975, Val Loss: 0.0754, Grad Norm: 0.6688\n",
      "Epoch 176/2001, Time Elapsed: 1046.54s, Train Loss: 0.0933, Val Loss: 0.0791, Grad Norm: 0.6536\n",
      "Early stopping with best val loss: 0.06883936105617161!\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train_diffusion_prior(\n",
    "                model=diffusion_prior_model,\n",
    "                noise_scheduler=noise_scheduler,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=test_dataloader,\n",
    "                optimizer=diffusion_optimizer,\n",
    "                scheduler=scheduler,\n",
    "                num_unique_users=len(unique_users),\n",
    "                objective=\"noise-pred\",\n",
    "                device=device,\n",
    "                num_epochs=2001,      # Ensure config.num_epochs is defined\n",
    "                patience=50,\n",
    "                savepath=savepath,\n",
    "                return_losses=True,\n",
    "                verbose=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or we may run large-scale experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'timesteps': [6000],\n",
    "    'layers': [8],\n",
    "    'heads': [32],\n",
    "    'dim_feedforward':[1024, 2048],\n",
    "    'num_image_tokens': [1],\n",
    "    'num_user_tokens': [4],\n",
    "    'learning_rate': [1e-4],\n",
    "    #'optimizers': ['adamw', 'sgd'],\n",
    "    'optimizers': ['adamw'],\n",
    "    #'schedulers': ['reduce_on_plateau', 'cosine'],\n",
    "    'schedulers': ['reduce_on_plateau'],\n",
    "    'batch_size': [64, 128],\n",
    "    #'noise_schedule': ['linear', \"squaredcos_cap_v2\"],\n",
    "    'noise_schedule': ['linear'],\n",
    "    'samples_per_user': [50, 80, 110, 140],\n",
    "    'objective':[\"noise-pred\"],\n",
    "    'use_ue': [True],\n",
    "    'img_embed_dim': [1280]\n",
    "}\n",
    "\n",
    "savedir = \"../data/flickr/evaluation/diffusion_priors/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter combinations:   0%|          | 0/16 [00:00<?, ?it/s]c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=linear, samples_per_user=50, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter combinations:   6%|▋         | 1/16 [13:51<3:27:56, 831.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.07189387866649134!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=linear, samples_per_user=80, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  12%|█▎        | 2/16 [41:16<5:05:36, 1309.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.06602152019482234!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=linear, samples_per_user=110, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  19%|█▉        | 3/16 [1:14:29<5:51:23, 1621.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.0629817265374907!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=linear, samples_per_user=140, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  25%|██▌       | 4/16 [1:44:05<5:36:31, 1682.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.0573450588332168!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=128, noise_schedule=linear, samples_per_user=50, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  31%|███▏      | 5/16 [2:00:04<4:20:40, 1421.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.07419471666216851!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=128, noise_schedule=linear, samples_per_user=80, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  38%|███▊      | 6/16 [2:15:31<3:28:56, 1253.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.068526175369819!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=128, noise_schedule=linear, samples_per_user=110, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  44%|████▍     | 7/16 [2:41:18<3:22:26, 1349.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.062235010663668315!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=128, noise_schedule=linear, samples_per_user=140, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  50%|█████     | 8/16 [3:08:31<3:11:57, 1439.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.061932186037302016!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=linear, samples_per_user=50, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  56%|█████▋    | 9/16 [3:24:25<2:30:15, 1287.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.0725581044780797!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=linear, samples_per_user=80, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  62%|██████▎   | 10/16 [3:55:37<2:26:49, 1468.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.06175178456409224!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=linear, samples_per_user=110, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  69%|██████▉   | 11/16 [4:23:55<2:08:12, 1538.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.05620144847138175!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=linear, samples_per_user=140, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  75%|███████▌  | 12/16 [5:14:44<2:13:12, 1998.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.057925735311261536!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=128, noise_schedule=linear, samples_per_user=50, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  81%|████████▏ | 13/16 [5:33:13<1:26:25, 1728.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.07841355130076408!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=128, noise_schedule=linear, samples_per_user=80, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  88%|████████▊ | 14/16 [5:50:00<50:21, 1510.80s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.06790840278069178!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=128, noise_schedule=linear, samples_per_user=110, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations:  94%|█████████▍| 15/16 [6:17:10<25:46, 1546.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.0736171322564284!\n",
      "Running configuration: timesteps=6000, layers=8, heads=32, image_tokens=1, user_tokens=4, learning_rate=0.0001, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=128, noise_schedule=linear, samples_per_user=140, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\recgensys-env\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Hyperparameter combinations: 100%|██████████| 16/16 [6:48:01<00:00, 1530.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.062047504385312396!\n",
      "Experimentation complete. Results saved to results.csv at ../data/flickr/evaluation/diffusion_priors/models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_grid_search(\n",
    "    train_df=train_df,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    param_grid=param_grid,\n",
    "    savedir=savedir,\n",
    "    unique_users = len(unique_users)\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recgensys-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
