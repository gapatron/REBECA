{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom Diffusion Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from Datasets import RecommenderUserSampler, EmbeddingsDataset\n",
    "from grid_search import run_grid_search\n",
    "from prior_models import TransformerEmbeddingDiffusionModelv2\n",
    "from train_priors import train_diffusion_prior\n",
    "from utils import map_embeddings_to_ratings, split_recommender_data, set_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the data in its corresponding (sub)directory and map image embeddings to observations.\n",
    "The data in ratings.csv will constitute our observations, and for our purposes, it will \n",
    "consist of the triplets $(U_i, S_j, I_k)$, where $U_i$ corresponds user $i$, $S_j$ encodes wheter user likes $(\\text{ score}\\geq 4)$ or dislikes the image $(\\text{ score}< 4)$ and $I_k$ is the $k$-th image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = torch.load(\"../data/flickr/processed/ip-adapters/SD15/sd15_image_embeddings.pt\", weights_only=True)\n",
    "ratings_df = pd.read_csv(\"../data/flickr/processed/ratings.csv\")\n",
    "expanded_features = map_embeddings_to_ratings(image_features, ratings_df)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([193208, 1024])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User loss: 116\n",
      "Data loss: 7.281789573930686%\n"
     ]
    }
   ],
   "source": [
    "usr_threshold = 100\n",
    "\n",
    "liked_counts = (\n",
    "    ratings_df[ratings_df[\"score\"] >= 4]\n",
    "    .groupby(\"worker_id\")[\"score\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"liked_count\")\n",
    ")\n",
    "valid_users = liked_counts[liked_counts[\"liked_count\"] >= usr_threshold][\"worker_id\"].unique()\n",
    "valid_worker_id = liked_counts[liked_counts[\"liked_count\"] >= usr_threshold][\"worker_id\"].unique()\n",
    "filtered_ratings_df = ratings_df[ratings_df[\"worker_id\"].isin(valid_users)].copy()\n",
    "print(f\"User loss: {210-len(valid_users)}\")\n",
    "print(f\"Data loss: {100*(1 - filtered_ratings_df.shape[0]/ratings_df.shape[0])}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "210-116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_mapping = {old_id: new_id for new_id, old_id in enumerate(valid_worker_id)}\n",
    "filtered_ratings_df.rename(columns={\"worker_id\": \"old_worker_id\"}, inplace=True)\n",
    "filtered_ratings_df[\"worker_id\"] = filtered_ratings_df[\"old_worker_id\"].map(worker_mapping)\n",
    "#filtered_ratings_df = filtered_ratings_df.reset_index(drop=True)\n",
    "worker_mapping_df = pd.DataFrame(list(worker_mapping.items()), columns=[\"old_worker_id\", \"worker_id\"])\n",
    "worker_mapping_df.to_csv(f\"../data/flickr/processed/worker_id_mapping_usrthr_{usr_threshold}.csv\", index=False)\n",
    "filtered_ratings_df.to_csv(f\"../data/flickr/processed/filtered_ratings_df_usrthrs_{usr_threshold}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 177278\n",
      "Validation set size: 928\n",
      "Evaluation set size: 933\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = split_recommender_data(\n",
    "    ratings_df=filtered_ratings_df,\n",
    "    val_spu=10,\n",
    "    test_spu=10,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worker_id\n",
       "40      201\n",
       "36      208\n",
       "52      208\n",
       "72      210\n",
       "67      258\n",
       "      ...  \n",
       "49     8064\n",
       "20    11064\n",
       "22    11343\n",
       "87    17320\n",
       "28    17875\n",
       "Name: count, Length: 94, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['worker_id'].value_counts(ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(f\"../data/flickr/processed/train/train_usrthrs_{usr_threshold}.csv\", index=False)\n",
    "val_df.to_csv(f\"../data/flickr/processed/train/validation_usrthrs_{usr_threshold}.csv\", index=False)\n",
    "test_df.to_csv(f\"../data/flickr/processed/test/test_usrthrs_{usr_threshold}.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "torch.save(expanded_features[train_df.original_index], f\"../data/flickr/processed/train/train_ie_usrthrs_{usr_threshold}.pt\")\n",
    "torch.save(expanded_features[val_df.original_index], f\"../data/flickr/processed/train/validation_ie_usrthrs_{usr_threshold}.pt\")\n",
    "torch.save(expanded_features[test_df.original_index], f\"../data/flickr/processed/test/test_ie_usrthrs_{usr_threshold}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([177278, 1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_features[train_df.original_index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingsDataset(\n",
    "    train_df,\n",
    "    image_embeddings=expanded_features[train_df.original_index]\n",
    ")\n",
    "\n",
    "val_dataset = EmbeddingsDataset(\n",
    "    val_df,\n",
    "    image_embeddings=expanded_features[val_df.original_index]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prior_models import CrossAttentionDiffusionPrior\n",
    "\n",
    "model = CrossAttentionDiffusionPrior(\n",
    "    img_embed_dim=1024,\n",
    "    num_users=94,\n",
    "    num_tokens=16,\n",
    "    n_heads=16,\n",
    "    num_layers=16,\n",
    "    dim_feedforward=2048,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4806912\n",
      "Trainable parameters: 4806912\n"
     ]
    }
   ],
   "source": [
    "set_seeds(0)\n",
    "batch_size = 64\n",
    "samples_per_user = 500\n",
    "learning_rate = 1e-5\n",
    "unique_users = filtered_ratings_df[\"worker_id\"].unique()\n",
    "train_user_sampler = RecommenderUserSampler(train_df, num_users=len(unique_users), samples_per_user=samples_per_user)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_user_sampler, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "diffusion_optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=6000, beta_schedule=\"laplace\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(diffusion_optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "savepath = f\"../data/flickr/evaluation/diffusion_priors/models/weights/test_xattn_v4.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001, Time Elapsed: 8.70s, Train Loss: 0.7554, Val Loss: 0.8231, Grad Norm: 1.1109\n",
      "Epoch 2/2001, Time Elapsed: 17.46s, Train Loss: 0.7332, Val Loss: 0.7943, Grad Norm: 0.8083\n",
      "Epoch 3/2001, Time Elapsed: 26.06s, Train Loss: 0.7047, Val Loss: 0.7688, Grad Norm: 1.0431\n",
      "Epoch 4/2001, Time Elapsed: 34.69s, Train Loss: 0.6919, Val Loss: 0.7561, Grad Norm: 1.0565\n",
      "Epoch 5/2001, Time Elapsed: 43.27s, Train Loss: 0.6842, Val Loss: 0.7558, Grad Norm: 1.0783\n",
      "Epoch 6/2001, Time Elapsed: 51.90s, Train Loss: 0.6779, Val Loss: 0.7389, Grad Norm: 1.0854\n",
      "Epoch 7/2001, Time Elapsed: 60.46s, Train Loss: 0.6724, Val Loss: 0.7344, Grad Norm: 1.1183\n",
      "Epoch 8/2001, Time Elapsed: 69.15s, Train Loss: 0.6673, Val Loss: 0.7306, Grad Norm: 1.1258\n",
      "Epoch 9/2001, Time Elapsed: 77.83s, Train Loss: 0.6631, Val Loss: 0.7272, Grad Norm: 1.1120\n",
      "Epoch 10/2001, Time Elapsed: 86.47s, Train Loss: 0.6608, Val Loss: 0.7264, Grad Norm: 1.1082\n",
      "Epoch 11/2001, Time Elapsed: 94.98s, Train Loss: 0.6574, Val Loss: 0.7187, Grad Norm: 1.1078\n",
      "Epoch 12/2001, Time Elapsed: 103.62s, Train Loss: 0.6550, Val Loss: 0.7133, Grad Norm: 1.0984\n",
      "Epoch 13/2001, Time Elapsed: 112.21s, Train Loss: 0.6521, Val Loss: 0.7125, Grad Norm: 1.0945\n",
      "Epoch 14/2001, Time Elapsed: 120.74s, Train Loss: 0.6500, Val Loss: 0.7094, Grad Norm: 1.1076\n",
      "Epoch 15/2001, Time Elapsed: 129.43s, Train Loss: 0.6474, Val Loss: 0.7083, Grad Norm: 1.1050\n",
      "Epoch 16/2001, Time Elapsed: 138.06s, Train Loss: 0.6452, Val Loss: 0.7087, Grad Norm: 1.0996\n",
      "Epoch 17/2001, Time Elapsed: 146.60s, Train Loss: 0.6440, Val Loss: 0.7064, Grad Norm: 1.0939\n",
      "Epoch 18/2001, Time Elapsed: 155.49s, Train Loss: 0.6418, Val Loss: 0.7049, Grad Norm: 1.0970\n",
      "Epoch 19/2001, Time Elapsed: 164.17s, Train Loss: 0.6406, Val Loss: 0.6923, Grad Norm: 1.0897\n",
      "Epoch 20/2001, Time Elapsed: 172.85s, Train Loss: 0.6385, Val Loss: 0.6942, Grad Norm: 1.0803\n",
      "Epoch 21/2001, Time Elapsed: 181.37s, Train Loss: 0.6380, Val Loss: 0.6975, Grad Norm: 1.0728\n",
      "Epoch 22/2001, Time Elapsed: 189.91s, Train Loss: 0.6366, Val Loss: 0.6944, Grad Norm: 1.0662\n",
      "Epoch 23/2001, Time Elapsed: 198.48s, Train Loss: 0.6349, Val Loss: 0.6969, Grad Norm: 1.0611\n",
      "Epoch 24/2001, Time Elapsed: 207.03s, Train Loss: 0.6335, Val Loss: 0.6940, Grad Norm: 1.0593\n",
      "Epoch 25/2001, Time Elapsed: 215.57s, Train Loss: 0.6325, Val Loss: 0.6878, Grad Norm: 1.0537\n",
      "Epoch 26/2001, Time Elapsed: 224.16s, Train Loss: 0.6316, Val Loss: 0.6968, Grad Norm: 1.0565\n",
      "Epoch 27/2001, Time Elapsed: 232.70s, Train Loss: 0.6304, Val Loss: 0.6921, Grad Norm: 1.0587\n",
      "Epoch 28/2001, Time Elapsed: 241.23s, Train Loss: 0.6302, Val Loss: 0.6816, Grad Norm: 1.0502\n",
      "Epoch 29/2001, Time Elapsed: 249.86s, Train Loss: 0.6288, Val Loss: 0.6876, Grad Norm: 1.0510\n",
      "Epoch 30/2001, Time Elapsed: 258.38s, Train Loss: 0.6286, Val Loss: 0.6866, Grad Norm: 1.0492\n",
      "Epoch 31/2001, Time Elapsed: 266.90s, Train Loss: 0.6272, Val Loss: 0.6820, Grad Norm: 1.0556\n",
      "Epoch 32/2001, Time Elapsed: 275.44s, Train Loss: 0.6263, Val Loss: 0.6862, Grad Norm: 1.0577\n",
      "Epoch 33/2001, Time Elapsed: 284.01s, Train Loss: 0.6256, Val Loss: 0.6850, Grad Norm: 1.0523\n",
      "Epoch 34/2001, Time Elapsed: 292.73s, Train Loss: 0.6248, Val Loss: 0.6892, Grad Norm: 1.0582\n",
      "Epoch 35/2001, Time Elapsed: 301.32s, Train Loss: 0.6232, Val Loss: 0.6855, Grad Norm: 1.0209\n",
      "Epoch 36/2001, Time Elapsed: 309.92s, Train Loss: 0.6237, Val Loss: 0.6844, Grad Norm: 1.0209\n",
      "Epoch 37/2001, Time Elapsed: 318.52s, Train Loss: 0.6230, Val Loss: 0.6830, Grad Norm: 1.0221\n",
      "Epoch 38/2001, Time Elapsed: 327.10s, Train Loss: 0.6234, Val Loss: 0.6800, Grad Norm: 1.0273\n",
      "Epoch 39/2001, Time Elapsed: 335.71s, Train Loss: 0.6220, Val Loss: 0.6848, Grad Norm: 1.0256\n",
      "Epoch 40/2001, Time Elapsed: 344.28s, Train Loss: 0.6214, Val Loss: 0.6747, Grad Norm: 1.0288\n",
      "Epoch 41/2001, Time Elapsed: 352.89s, Train Loss: 0.6213, Val Loss: 0.6839, Grad Norm: 1.0263\n",
      "Epoch 42/2001, Time Elapsed: 361.41s, Train Loss: 0.6201, Val Loss: 0.6767, Grad Norm: 1.0245\n",
      "Epoch 43/2001, Time Elapsed: 369.97s, Train Loss: 0.6208, Val Loss: 0.6814, Grad Norm: 1.0238\n",
      "Epoch 44/2001, Time Elapsed: 378.51s, Train Loss: 0.6197, Val Loss: 0.6818, Grad Norm: 1.0257\n",
      "Epoch 45/2001, Time Elapsed: 387.05s, Train Loss: 0.6193, Val Loss: 0.6787, Grad Norm: 1.0287\n",
      "Epoch 46/2001, Time Elapsed: 395.57s, Train Loss: 0.6190, Val Loss: 0.6801, Grad Norm: 1.0288\n",
      "Epoch 47/2001, Time Elapsed: 404.12s, Train Loss: 0.6188, Val Loss: 0.6733, Grad Norm: 1.0130\n",
      "Epoch 48/2001, Time Elapsed: 412.85s, Train Loss: 0.6187, Val Loss: 0.6782, Grad Norm: 1.0124\n",
      "Epoch 49/2001, Time Elapsed: 421.47s, Train Loss: 0.6188, Val Loss: 0.6782, Grad Norm: 1.0141\n",
      "Epoch 50/2001, Time Elapsed: 430.17s, Train Loss: 0.6180, Val Loss: 0.6817, Grad Norm: 1.0136\n",
      "Epoch 51/2001, Time Elapsed: 438.84s, Train Loss: 0.6180, Val Loss: 0.6785, Grad Norm: 1.0147\n",
      "Epoch 52/2001, Time Elapsed: 447.49s, Train Loss: 0.6178, Val Loss: 0.6764, Grad Norm: 1.0154\n",
      "Epoch 53/2001, Time Elapsed: 456.08s, Train Loss: 0.6182, Val Loss: 0.6692, Grad Norm: 1.0146\n",
      "Epoch 54/2001, Time Elapsed: 464.65s, Train Loss: 0.6175, Val Loss: 0.6775, Grad Norm: 1.0172\n",
      "Epoch 55/2001, Time Elapsed: 473.20s, Train Loss: 0.6175, Val Loss: 0.6774, Grad Norm: 1.0184\n",
      "Epoch 56/2001, Time Elapsed: 481.71s, Train Loss: 0.6175, Val Loss: 0.6739, Grad Norm: 1.0215\n",
      "Epoch 57/2001, Time Elapsed: 490.20s, Train Loss: 0.6166, Val Loss: 0.6781, Grad Norm: 1.0224\n",
      "Epoch 58/2001, Time Elapsed: 498.93s, Train Loss: 0.6158, Val Loss: 0.6759, Grad Norm: 1.0236\n",
      "Epoch 59/2001, Time Elapsed: 507.92s, Train Loss: 0.6158, Val Loss: 0.6746, Grad Norm: 1.0256\n",
      "Epoch 60/2001, Time Elapsed: 516.63s, Train Loss: 0.6161, Val Loss: 0.6805, Grad Norm: 1.0116\n",
      "Epoch 61/2001, Time Elapsed: 525.13s, Train Loss: 0.6163, Val Loss: 0.6781, Grad Norm: 1.0084\n",
      "Epoch 62/2001, Time Elapsed: 533.57s, Train Loss: 0.6160, Val Loss: 0.6718, Grad Norm: 1.0138\n",
      "Epoch 63/2001, Time Elapsed: 542.03s, Train Loss: 0.6165, Val Loss: 0.6748, Grad Norm: 1.0192\n",
      "Epoch 64/2001, Time Elapsed: 550.47s, Train Loss: 0.6153, Val Loss: 0.6754, Grad Norm: 1.0159\n",
      "Epoch 65/2001, Time Elapsed: 558.91s, Train Loss: 0.6148, Val Loss: 0.6728, Grad Norm: 1.0147\n",
      "Epoch 66/2001, Time Elapsed: 567.49s, Train Loss: 0.6151, Val Loss: 0.6745, Grad Norm: 1.0096\n",
      "Epoch 67/2001, Time Elapsed: 575.97s, Train Loss: 0.6151, Val Loss: 0.6767, Grad Norm: 1.0123\n",
      "Epoch 68/2001, Time Elapsed: 584.45s, Train Loss: 0.6154, Val Loss: 0.6771, Grad Norm: 1.0109\n",
      "Epoch 69/2001, Time Elapsed: 592.84s, Train Loss: 0.6150, Val Loss: 0.6709, Grad Norm: 1.0082\n",
      "Epoch 70/2001, Time Elapsed: 601.42s, Train Loss: 0.6152, Val Loss: 0.6729, Grad Norm: 1.0108\n",
      "Epoch 71/2001, Time Elapsed: 610.03s, Train Loss: 0.6149, Val Loss: 0.6736, Grad Norm: 1.0113\n",
      "Epoch 72/2001, Time Elapsed: 618.71s, Train Loss: 0.6154, Val Loss: 0.6723, Grad Norm: 1.0058\n",
      "Epoch 73/2001, Time Elapsed: 627.34s, Train Loss: 0.6156, Val Loss: 0.6656, Grad Norm: 1.0074\n",
      "Epoch 74/2001, Time Elapsed: 636.10s, Train Loss: 0.6153, Val Loss: 0.6734, Grad Norm: 1.0075\n",
      "Epoch 75/2001, Time Elapsed: 644.78s, Train Loss: 0.6148, Val Loss: 0.6736, Grad Norm: 1.0081\n",
      "Epoch 76/2001, Time Elapsed: 653.36s, Train Loss: 0.6152, Val Loss: 0.6749, Grad Norm: 1.0093\n",
      "Epoch 77/2001, Time Elapsed: 661.92s, Train Loss: 0.6148, Val Loss: 0.6720, Grad Norm: 1.0039\n",
      "Epoch 78/2001, Time Elapsed: 670.50s, Train Loss: 0.6147, Val Loss: 0.6781, Grad Norm: 1.0060\n",
      "Epoch 79/2001, Time Elapsed: 679.05s, Train Loss: 0.6155, Val Loss: 0.6759, Grad Norm: 1.0098\n",
      "Epoch 80/2001, Time Elapsed: 687.64s, Train Loss: 0.6149, Val Loss: 0.6722, Grad Norm: 1.0074\n",
      "Epoch 81/2001, Time Elapsed: 696.17s, Train Loss: 0.6155, Val Loss: 0.6762, Grad Norm: 1.0051\n",
      "Epoch 82/2001, Time Elapsed: 704.90s, Train Loss: 0.6145, Val Loss: 0.6727, Grad Norm: 1.0056\n",
      "Epoch 83/2001, Time Elapsed: 713.48s, Train Loss: 0.6144, Val Loss: 0.6706, Grad Norm: 1.0061\n",
      "Epoch 84/2001, Time Elapsed: 722.04s, Train Loss: 0.6146, Val Loss: 0.6735, Grad Norm: 1.0070\n",
      "Epoch 85/2001, Time Elapsed: 730.62s, Train Loss: 0.6146, Val Loss: 0.6712, Grad Norm: 1.0066\n",
      "Epoch 86/2001, Time Elapsed: 739.13s, Train Loss: 0.6149, Val Loss: 0.6722, Grad Norm: 1.0069\n",
      "Epoch 87/2001, Time Elapsed: 747.71s, Train Loss: 0.6150, Val Loss: 0.6756, Grad Norm: 1.0049\n",
      "Epoch 88/2001, Time Elapsed: 756.28s, Train Loss: 0.6151, Val Loss: 0.6775, Grad Norm: 1.0042\n",
      "Epoch 89/2001, Time Elapsed: 765.01s, Train Loss: 0.6150, Val Loss: 0.6749, Grad Norm: 1.0067\n",
      "Epoch 90/2001, Time Elapsed: 773.60s, Train Loss: 0.6146, Val Loss: 0.6719, Grad Norm: 1.0085\n",
      "Epoch 91/2001, Time Elapsed: 782.16s, Train Loss: 0.6149, Val Loss: 0.6726, Grad Norm: 1.0060\n",
      "Epoch 92/2001, Time Elapsed: 790.72s, Train Loss: 0.6148, Val Loss: 0.6761, Grad Norm: 1.0094\n",
      "Epoch 93/2001, Time Elapsed: 799.29s, Train Loss: 0.6148, Val Loss: 0.6727, Grad Norm: 1.0050\n",
      "Early stopping with best val loss: 0.6656282345453898!\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train_diffusion_prior(\n",
    "                model=model,\n",
    "                noise_scheduler=noise_scheduler,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=test_dataloader,\n",
    "                optimizer=diffusion_optimizer,\n",
    "                scheduler=scheduler,\n",
    "                num_unique_users=len(unique_users),\n",
    "                objective=\"v_prediction\",\n",
    "                device=device,\n",
    "                num_epochs=2001,      # Ensure config.num_epochs is defined\n",
    "                patience=20,\n",
    "                savepath=savepath,\n",
    "                return_losses=True,\n",
    "                verbose=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/rebeca-env/lib/python3.13/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "diffusion_prior_model = TransformerEmbeddingDiffusionModelv2(\n",
    "    img_embed_dim=1024,\n",
    "    num_users=122,    # So user embedding covers your entire user set\n",
    "    n_heads=16,\n",
    "    num_tokens=1,\n",
    "    num_user_tokens=4,\n",
    "    num_layers=8,\n",
    "    dim_feedforward=2048,\n",
    "    whether_use_user_embeddings=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 68756480\n",
      "Trainable parameters: 68756480\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "set_seeds(0)\n",
    "\n",
    "d = image_features.shape[-1]\n",
    "norms = image_features.norm(dim=-1, keepdim=True)\n",
    "norms = torch.clamp(norms, min=1e-8)\n",
    "image_features_normed = image_features / norms * math.sqrt(d)\n",
    "emb_final  = torch.clamp(image_features_normed, -3.2, 3.2) / 3.2   \n",
    "\n",
    "expanded_features = map_embeddings_to_ratings(emb_final, ratings_df)\n",
    "batch_size = 64\n",
    "samples_per_user = 80\n",
    "learning_rate = 1e-5\n",
    "unique_users = filtered_ratings_df[\"worker_id\"].unique()\n",
    "train_dataset = EmbeddingsDataset(\n",
    "        train_df,\n",
    "        image_embeddings=expanded_features[train_df.original_index]\n",
    "    )\n",
    "val_dataset = EmbeddingsDataset(\n",
    "        val_df,\n",
    "        image_embeddings=expanded_features[val_df.original_index]\n",
    "    )\n",
    "train_user_sampler = RecommenderUserSampler(train_df, num_users=len(unique_users), samples_per_user=samples_per_user)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_user_sampler, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "diffusion_optimizer = torch.optim.AdamW(diffusion_prior_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=6000, beta_schedule=\"laplace\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(diffusion_optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "total_params = sum(p.numel() for p in diffusion_prior_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in diffusion_prior_model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "savepath = f\"../data/flickr/evaluation/diffusion_priors/models/weights/test_rebecca_og_norm_v3.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 68756480\n",
      "Trainable parameters: 68756480\n"
     ]
    }
   ],
   "source": [
    "set_seeds(0)\n",
    "batch_size = 64\n",
    "samples_per_user = 80\n",
    "learning_rate = 1e-4\n",
    "unique_users = filtered_ratings_df[\"worker_id\"].unique()\n",
    "train_user_sampler = RecommenderUserSampler(train_df, num_users=len(unique_users), samples_per_user=samples_per_user)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_user_sampler, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "diffusion_optimizer = torch.optim.AdamW(diffusion_prior_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=6000)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(diffusion_optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "total_params = sum(p.numel() for p in diffusion_prior_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in diffusion_prior_model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "\n",
    "\n",
    "savepath = f\"../data/flickr/evaluation/diffusion_priors/models/weights/sd15_ied1024_nu122_nh16_nit1_nut4_nl8_dff2048.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001, Time Elapsed: 2.26s, Train Loss: 1.6912, Val Loss: 1.0891, Grad Norm: 7.7146\n",
      "Epoch 2/2001, Time Elapsed: 4.70s, Train Loss: 1.1028, Val Loss: 0.9466, Grad Norm: 5.0367\n",
      "Epoch 3/2001, Time Elapsed: 7.23s, Train Loss: 1.0253, Val Loss: 0.9138, Grad Norm: 4.4878\n",
      "Epoch 4/2001, Time Elapsed: 9.75s, Train Loss: 0.9836, Val Loss: 0.8918, Grad Norm: 3.9995\n",
      "Epoch 5/2001, Time Elapsed: 12.26s, Train Loss: 0.9566, Val Loss: 0.8733, Grad Norm: 3.7429\n",
      "Epoch 6/2001, Time Elapsed: 14.79s, Train Loss: 0.9345, Val Loss: 0.8610, Grad Norm: 3.5570\n",
      "Epoch 7/2001, Time Elapsed: 17.42s, Train Loss: 0.9140, Val Loss: 0.8551, Grad Norm: 3.3351\n",
      "Epoch 8/2001, Time Elapsed: 20.23s, Train Loss: 0.8980, Val Loss: 0.8401, Grad Norm: 3.1974\n",
      "Epoch 9/2001, Time Elapsed: 22.74s, Train Loss: 0.8837, Val Loss: 0.8387, Grad Norm: 3.0305\n",
      "Epoch 10/2001, Time Elapsed: 25.27s, Train Loss: 0.8731, Val Loss: 0.8288, Grad Norm: 2.9270\n",
      "Epoch 11/2001, Time Elapsed: 27.77s, Train Loss: 0.8615, Val Loss: 0.8170, Grad Norm: 2.8276\n",
      "Epoch 12/2001, Time Elapsed: 30.29s, Train Loss: 0.8502, Val Loss: 0.8094, Grad Norm: 2.7497\n",
      "Epoch 13/2001, Time Elapsed: 32.80s, Train Loss: 0.8397, Val Loss: 0.8001, Grad Norm: 2.6440\n",
      "Epoch 14/2001, Time Elapsed: 35.32s, Train Loss: 0.8320, Val Loss: 0.7976, Grad Norm: 2.6021\n",
      "Epoch 15/2001, Time Elapsed: 37.84s, Train Loss: 0.8207, Val Loss: 0.7887, Grad Norm: 2.5131\n",
      "Epoch 16/2001, Time Elapsed: 40.37s, Train Loss: 0.8105, Val Loss: 0.7780, Grad Norm: 2.4623\n",
      "Epoch 17/2001, Time Elapsed: 42.89s, Train Loss: 0.8002, Val Loss: 0.7612, Grad Norm: 2.3980\n",
      "Epoch 18/2001, Time Elapsed: 45.42s, Train Loss: 0.7899, Val Loss: 0.7554, Grad Norm: 2.3436\n",
      "Epoch 19/2001, Time Elapsed: 47.94s, Train Loss: 0.7790, Val Loss: 0.7494, Grad Norm: 2.3036\n",
      "Epoch 20/2001, Time Elapsed: 50.48s, Train Loss: 0.7700, Val Loss: 0.7374, Grad Norm: 2.2483\n",
      "Epoch 21/2001, Time Elapsed: 52.98s, Train Loss: 0.7587, Val Loss: 0.7289, Grad Norm: 2.1957\n",
      "Epoch 22/2001, Time Elapsed: 55.49s, Train Loss: 0.7462, Val Loss: 0.7196, Grad Norm: 2.1693\n",
      "Epoch 23/2001, Time Elapsed: 58.00s, Train Loss: 0.7389, Val Loss: 0.7081, Grad Norm: 2.1319\n",
      "Epoch 24/2001, Time Elapsed: 60.52s, Train Loss: 0.7253, Val Loss: 0.6964, Grad Norm: 2.0933\n",
      "Epoch 25/2001, Time Elapsed: 63.02s, Train Loss: 0.7170, Val Loss: 0.6909, Grad Norm: 2.0590\n",
      "Epoch 26/2001, Time Elapsed: 65.52s, Train Loss: 0.7066, Val Loss: 0.6787, Grad Norm: 2.0471\n",
      "Epoch 27/2001, Time Elapsed: 68.02s, Train Loss: 0.6974, Val Loss: 0.6693, Grad Norm: 2.0381\n",
      "Epoch 28/2001, Time Elapsed: 70.53s, Train Loss: 0.6867, Val Loss: 0.6629, Grad Norm: 1.9974\n",
      "Epoch 29/2001, Time Elapsed: 73.05s, Train Loss: 0.6774, Val Loss: 0.6450, Grad Norm: 1.9977\n",
      "Epoch 30/2001, Time Elapsed: 75.56s, Train Loss: 0.6679, Val Loss: 0.6360, Grad Norm: 1.9680\n",
      "Epoch 31/2001, Time Elapsed: 78.05s, Train Loss: 0.6583, Val Loss: 0.6298, Grad Norm: 1.9580\n",
      "Epoch 32/2001, Time Elapsed: 80.56s, Train Loss: 0.6489, Val Loss: 0.6184, Grad Norm: 1.9422\n",
      "Epoch 33/2001, Time Elapsed: 83.16s, Train Loss: 0.6391, Val Loss: 0.6056, Grad Norm: 1.9313\n",
      "Epoch 34/2001, Time Elapsed: 86.09s, Train Loss: 0.6320, Val Loss: 0.6032, Grad Norm: 1.9166\n",
      "Epoch 35/2001, Time Elapsed: 88.59s, Train Loss: 0.6219, Val Loss: 0.5864, Grad Norm: 1.9040\n",
      "Epoch 36/2001, Time Elapsed: 91.09s, Train Loss: 0.6142, Val Loss: 0.5825, Grad Norm: 1.8941\n",
      "Epoch 37/2001, Time Elapsed: 93.60s, Train Loss: 0.6048, Val Loss: 0.5731, Grad Norm: 1.8808\n",
      "Epoch 38/2001, Time Elapsed: 96.11s, Train Loss: 0.5982, Val Loss: 0.5656, Grad Norm: 1.8758\n",
      "Epoch 39/2001, Time Elapsed: 98.61s, Train Loss: 0.5914, Val Loss: 0.5594, Grad Norm: 1.8519\n",
      "Epoch 40/2001, Time Elapsed: 101.12s, Train Loss: 0.5848, Val Loss: 0.5500, Grad Norm: 1.8590\n",
      "Epoch 41/2001, Time Elapsed: 103.64s, Train Loss: 0.5760, Val Loss: 0.5360, Grad Norm: 1.8424\n",
      "Epoch 42/2001, Time Elapsed: 106.14s, Train Loss: 0.5692, Val Loss: 0.5345, Grad Norm: 1.8268\n",
      "Epoch 43/2001, Time Elapsed: 108.66s, Train Loss: 0.5632, Val Loss: 0.5291, Grad Norm: 1.8180\n",
      "Epoch 44/2001, Time Elapsed: 111.17s, Train Loss: 0.5569, Val Loss: 0.5151, Grad Norm: 1.8094\n",
      "Epoch 45/2001, Time Elapsed: 113.68s, Train Loss: 0.5506, Val Loss: 0.5101, Grad Norm: 1.7956\n",
      "Epoch 46/2001, Time Elapsed: 116.19s, Train Loss: 0.5443, Val Loss: 0.5068, Grad Norm: 1.7918\n",
      "Epoch 47/2001, Time Elapsed: 118.69s, Train Loss: 0.5403, Val Loss: 0.4997, Grad Norm: 1.7814\n",
      "Epoch 48/2001, Time Elapsed: 121.19s, Train Loss: 0.5353, Val Loss: 0.4927, Grad Norm: 1.7678\n",
      "Epoch 49/2001, Time Elapsed: 123.69s, Train Loss: 0.5308, Val Loss: 0.4898, Grad Norm: 1.7667\n",
      "Epoch 50/2001, Time Elapsed: 126.20s, Train Loss: 0.5241, Val Loss: 0.4800, Grad Norm: 1.7575\n",
      "Epoch 51/2001, Time Elapsed: 128.70s, Train Loss: 0.5182, Val Loss: 0.4772, Grad Norm: 1.7461\n",
      "Epoch 52/2001, Time Elapsed: 131.23s, Train Loss: 0.5170, Val Loss: 0.4691, Grad Norm: 1.7431\n",
      "Epoch 53/2001, Time Elapsed: 133.75s, Train Loss: 0.5111, Val Loss: 0.4687, Grad Norm: 1.7365\n",
      "Epoch 54/2001, Time Elapsed: 136.27s, Train Loss: 0.5042, Val Loss: 0.4598, Grad Norm: 1.7288\n",
      "Epoch 55/2001, Time Elapsed: 138.77s, Train Loss: 0.5024, Val Loss: 0.4623, Grad Norm: 1.7237\n",
      "Epoch 56/2001, Time Elapsed: 141.03s, Train Loss: 0.4972, Val Loss: 0.4458, Grad Norm: 1.7143\n",
      "Epoch 57/2001, Time Elapsed: 143.54s, Train Loss: 0.4904, Val Loss: 0.4444, Grad Norm: 1.6989\n",
      "Epoch 58/2001, Time Elapsed: 146.06s, Train Loss: 0.4893, Val Loss: 0.4411, Grad Norm: 1.7035\n",
      "Epoch 59/2001, Time Elapsed: 148.57s, Train Loss: 0.4848, Val Loss: 0.4373, Grad Norm: 1.6878\n",
      "Epoch 60/2001, Time Elapsed: 151.09s, Train Loss: 0.4824, Val Loss: 0.4341, Grad Norm: 1.6938\n",
      "Epoch 61/2001, Time Elapsed: 153.98s, Train Loss: 0.4759, Val Loss: 0.4264, Grad Norm: 1.6767\n",
      "Epoch 62/2001, Time Elapsed: 156.50s, Train Loss: 0.4715, Val Loss: 0.4221, Grad Norm: 1.6672\n",
      "Epoch 63/2001, Time Elapsed: 159.03s, Train Loss: 0.4694, Val Loss: 0.4216, Grad Norm: 1.6619\n",
      "Epoch 64/2001, Time Elapsed: 161.55s, Train Loss: 0.4661, Val Loss: 0.4157, Grad Norm: 1.6620\n",
      "Epoch 65/2001, Time Elapsed: 164.08s, Train Loss: 0.4633, Val Loss: 0.4135, Grad Norm: 1.6589\n",
      "Epoch 66/2001, Time Elapsed: 166.61s, Train Loss: 0.4602, Val Loss: 0.4133, Grad Norm: 1.6524\n",
      "Epoch 67/2001, Time Elapsed: 169.13s, Train Loss: 0.4547, Val Loss: 0.4053, Grad Norm: 1.6492\n",
      "Epoch 68/2001, Time Elapsed: 171.66s, Train Loss: 0.4536, Val Loss: 0.4049, Grad Norm: 1.6496\n",
      "Epoch 69/2001, Time Elapsed: 174.17s, Train Loss: 0.4511, Val Loss: 0.3986, Grad Norm: 1.6409\n",
      "Epoch 70/2001, Time Elapsed: 176.69s, Train Loss: 0.4461, Val Loss: 0.3924, Grad Norm: 1.6302\n",
      "Epoch 71/2001, Time Elapsed: 179.20s, Train Loss: 0.4464, Val Loss: 0.3923, Grad Norm: 1.6370\n",
      "Epoch 72/2001, Time Elapsed: 181.72s, Train Loss: 0.4412, Val Loss: 0.3907, Grad Norm: 1.6189\n",
      "Epoch 73/2001, Time Elapsed: 184.24s, Train Loss: 0.4377, Val Loss: 0.3870, Grad Norm: 1.6172\n",
      "Epoch 74/2001, Time Elapsed: 186.76s, Train Loss: 0.4355, Val Loss: 0.3874, Grad Norm: 1.6123\n",
      "Epoch 75/2001, Time Elapsed: 189.02s, Train Loss: 0.4326, Val Loss: 0.3784, Grad Norm: 1.6081\n",
      "Epoch 76/2001, Time Elapsed: 191.55s, Train Loss: 0.4279, Val Loss: 0.3743, Grad Norm: 1.5992\n",
      "Epoch 77/2001, Time Elapsed: 194.08s, Train Loss: 0.4271, Val Loss: 0.3767, Grad Norm: 1.5965\n",
      "Epoch 78/2001, Time Elapsed: 196.34s, Train Loss: 0.4253, Val Loss: 0.3710, Grad Norm: 1.5946\n",
      "Epoch 79/2001, Time Elapsed: 198.86s, Train Loss: 0.4218, Val Loss: 0.3697, Grad Norm: 1.5861\n",
      "Epoch 80/2001, Time Elapsed: 201.38s, Train Loss: 0.4187, Val Loss: 0.3714, Grad Norm: 1.5787\n",
      "Epoch 81/2001, Time Elapsed: 203.64s, Train Loss: 0.4181, Val Loss: 0.3655, Grad Norm: 1.5845\n",
      "Epoch 82/2001, Time Elapsed: 206.17s, Train Loss: 0.4128, Val Loss: 0.3648, Grad Norm: 1.5698\n",
      "Epoch 83/2001, Time Elapsed: 208.70s, Train Loss: 0.4119, Val Loss: 0.3586, Grad Norm: 1.5734\n",
      "Epoch 84/2001, Time Elapsed: 211.22s, Train Loss: 0.4110, Val Loss: 0.3539, Grad Norm: 1.5677\n",
      "Epoch 85/2001, Time Elapsed: 213.72s, Train Loss: 0.4061, Val Loss: 0.3547, Grad Norm: 1.5631\n",
      "Epoch 86/2001, Time Elapsed: 215.96s, Train Loss: 0.4046, Val Loss: 0.3475, Grad Norm: 1.5571\n",
      "Epoch 87/2001, Time Elapsed: 218.47s, Train Loss: 0.4023, Val Loss: 0.3501, Grad Norm: 1.5501\n",
      "Epoch 88/2001, Time Elapsed: 220.73s, Train Loss: 0.4015, Val Loss: 0.3499, Grad Norm: 1.5516\n",
      "Epoch 89/2001, Time Elapsed: 222.96s, Train Loss: 0.3994, Val Loss: 0.3462, Grad Norm: 1.5461\n",
      "Epoch 90/2001, Time Elapsed: 225.46s, Train Loss: 0.3952, Val Loss: 0.3452, Grad Norm: 1.5362\n",
      "Epoch 91/2001, Time Elapsed: 227.95s, Train Loss: 0.3948, Val Loss: 0.3411, Grad Norm: 1.5347\n",
      "Epoch 92/2001, Time Elapsed: 230.70s, Train Loss: 0.3943, Val Loss: 0.3368, Grad Norm: 1.5373\n",
      "Epoch 93/2001, Time Elapsed: 233.54s, Train Loss: 0.3903, Val Loss: 0.3371, Grad Norm: 1.5270\n",
      "Epoch 94/2001, Time Elapsed: 235.79s, Train Loss: 0.3886, Val Loss: 0.3326, Grad Norm: 1.5265\n",
      "Epoch 95/2001, Time Elapsed: 238.30s, Train Loss: 0.3870, Val Loss: 0.3343, Grad Norm: 1.5227\n",
      "Epoch 96/2001, Time Elapsed: 240.54s, Train Loss: 0.3824, Val Loss: 0.3320, Grad Norm: 1.5131\n",
      "Epoch 97/2001, Time Elapsed: 243.04s, Train Loss: 0.3840, Val Loss: 0.3340, Grad Norm: 1.5189\n",
      "Epoch 98/2001, Time Elapsed: 245.28s, Train Loss: 0.3814, Val Loss: 0.3291, Grad Norm: 1.5126\n",
      "Epoch 99/2001, Time Elapsed: 247.79s, Train Loss: 0.3790, Val Loss: 0.3273, Grad Norm: 1.5057\n",
      "Epoch 100/2001, Time Elapsed: 250.30s, Train Loss: 0.3747, Val Loss: 0.3220, Grad Norm: 1.4980\n",
      "Epoch 101/2001, Time Elapsed: 252.81s, Train Loss: 0.3752, Val Loss: 0.3232, Grad Norm: 1.4978\n",
      "Epoch 102/2001, Time Elapsed: 255.05s, Train Loss: 0.3739, Val Loss: 0.3205, Grad Norm: 1.4968\n",
      "Epoch 103/2001, Time Elapsed: 257.56s, Train Loss: 0.3714, Val Loss: 0.3195, Grad Norm: 1.4886\n",
      "Epoch 104/2001, Time Elapsed: 260.07s, Train Loss: 0.3712, Val Loss: 0.3178, Grad Norm: 1.4884\n",
      "Epoch 105/2001, Time Elapsed: 262.57s, Train Loss: 0.3683, Val Loss: 0.3123, Grad Norm: 1.4850\n",
      "Epoch 106/2001, Time Elapsed: 265.07s, Train Loss: 0.3669, Val Loss: 0.3118, Grad Norm: 1.4788\n",
      "Epoch 107/2001, Time Elapsed: 267.58s, Train Loss: 0.3645, Val Loss: 0.3106, Grad Norm: 1.4715\n",
      "Epoch 108/2001, Time Elapsed: 270.08s, Train Loss: 0.3637, Val Loss: 0.3102, Grad Norm: 1.4747\n",
      "Epoch 109/2001, Time Elapsed: 272.59s, Train Loss: 0.3621, Val Loss: 0.3100, Grad Norm: 1.4724\n",
      "Epoch 110/2001, Time Elapsed: 275.08s, Train Loss: 0.3602, Val Loss: 0.3053, Grad Norm: 1.4661\n",
      "Epoch 111/2001, Time Elapsed: 277.58s, Train Loss: 0.3573, Val Loss: 0.3010, Grad Norm: 1.4581\n",
      "Epoch 112/2001, Time Elapsed: 280.08s, Train Loss: 0.3563, Val Loss: 0.3044, Grad Norm: 1.4595\n",
      "Epoch 113/2001, Time Elapsed: 282.32s, Train Loss: 0.3556, Val Loss: 0.3033, Grad Norm: 1.4574\n",
      "Epoch 114/2001, Time Elapsed: 284.57s, Train Loss: 0.3532, Val Loss: 0.3010, Grad Norm: 1.4504\n",
      "Epoch 115/2001, Time Elapsed: 286.80s, Train Loss: 0.3523, Val Loss: 0.3049, Grad Norm: 1.4449\n",
      "Epoch 116/2001, Time Elapsed: 289.04s, Train Loss: 0.3513, Val Loss: 0.3012, Grad Norm: 1.4473\n",
      "Epoch 117/2001, Time Elapsed: 291.28s, Train Loss: 0.3489, Val Loss: 0.2973, Grad Norm: 1.4404\n",
      "Epoch 118/2001, Time Elapsed: 293.79s, Train Loss: 0.3457, Val Loss: 0.2966, Grad Norm: 1.4308\n",
      "Epoch 119/2001, Time Elapsed: 296.30s, Train Loss: 0.3461, Val Loss: 0.2966, Grad Norm: 1.4332\n",
      "Epoch 120/2001, Time Elapsed: 298.55s, Train Loss: 0.3440, Val Loss: 0.2953, Grad Norm: 1.4277\n",
      "Epoch 121/2001, Time Elapsed: 301.05s, Train Loss: 0.3442, Val Loss: 0.2919, Grad Norm: 1.4286\n",
      "Epoch 122/2001, Time Elapsed: 303.56s, Train Loss: 0.3415, Val Loss: 0.2899, Grad Norm: 1.4240\n",
      "Epoch 123/2001, Time Elapsed: 306.06s, Train Loss: 0.3406, Val Loss: 0.2888, Grad Norm: 1.4202\n",
      "Epoch 124/2001, Time Elapsed: 308.57s, Train Loss: 0.3400, Val Loss: 0.2869, Grad Norm: 1.4199\n",
      "Epoch 125/2001, Time Elapsed: 311.10s, Train Loss: 0.3367, Val Loss: 0.2849, Grad Norm: 1.4145\n",
      "Epoch 126/2001, Time Elapsed: 313.61s, Train Loss: 0.3365, Val Loss: 0.2838, Grad Norm: 1.4141\n",
      "Epoch 127/2001, Time Elapsed: 316.10s, Train Loss: 0.3349, Val Loss: 0.2861, Grad Norm: 1.4096\n",
      "Epoch 128/2001, Time Elapsed: 318.34s, Train Loss: 0.3334, Val Loss: 0.2852, Grad Norm: 1.4050\n",
      "Epoch 129/2001, Time Elapsed: 320.58s, Train Loss: 0.3311, Val Loss: 0.2835, Grad Norm: 1.3957\n",
      "Epoch 130/2001, Time Elapsed: 323.22s, Train Loss: 0.3310, Val Loss: 0.2805, Grad Norm: 1.3998\n",
      "Epoch 131/2001, Time Elapsed: 326.01s, Train Loss: 0.3301, Val Loss: 0.2787, Grad Norm: 1.3986\n",
      "Epoch 132/2001, Time Elapsed: 328.51s, Train Loss: 0.3280, Val Loss: 0.2788, Grad Norm: 1.3927\n",
      "Epoch 133/2001, Time Elapsed: 330.74s, Train Loss: 0.3277, Val Loss: 0.2806, Grad Norm: 1.3903\n",
      "Epoch 134/2001, Time Elapsed: 332.99s, Train Loss: 0.3261, Val Loss: 0.2802, Grad Norm: 1.3900\n",
      "Epoch 135/2001, Time Elapsed: 335.23s, Train Loss: 0.3242, Val Loss: 0.2737, Grad Norm: 1.3862\n",
      "Epoch 136/2001, Time Elapsed: 337.74s, Train Loss: 0.3227, Val Loss: 0.2734, Grad Norm: 1.3833\n",
      "Epoch 137/2001, Time Elapsed: 340.24s, Train Loss: 0.3234, Val Loss: 0.2724, Grad Norm: 1.3844\n",
      "Epoch 138/2001, Time Elapsed: 342.74s, Train Loss: 0.3232, Val Loss: 0.2738, Grad Norm: 1.3856\n",
      "Epoch 139/2001, Time Elapsed: 344.97s, Train Loss: 0.3199, Val Loss: 0.2668, Grad Norm: 1.3741\n",
      "Epoch 140/2001, Time Elapsed: 347.48s, Train Loss: 0.3179, Val Loss: 0.2694, Grad Norm: 1.3696\n",
      "Epoch 141/2001, Time Elapsed: 349.71s, Train Loss: 0.3165, Val Loss: 0.2687, Grad Norm: 1.3637\n",
      "Epoch 142/2001, Time Elapsed: 351.95s, Train Loss: 0.3166, Val Loss: 0.2643, Grad Norm: 1.3676\n",
      "Epoch 143/2001, Time Elapsed: 354.44s, Train Loss: 0.3150, Val Loss: 0.2651, Grad Norm: 1.3651\n",
      "Epoch 144/2001, Time Elapsed: 356.69s, Train Loss: 0.3151, Val Loss: 0.2709, Grad Norm: 1.3622\n",
      "Epoch 145/2001, Time Elapsed: 358.92s, Train Loss: 0.3123, Val Loss: 0.2662, Grad Norm: 1.3551\n",
      "Epoch 146/2001, Time Elapsed: 361.16s, Train Loss: 0.3125, Val Loss: 0.2627, Grad Norm: 1.3580\n",
      "Epoch 147/2001, Time Elapsed: 363.67s, Train Loss: 0.3104, Val Loss: 0.2654, Grad Norm: 1.3526\n",
      "Epoch 148/2001, Time Elapsed: 365.91s, Train Loss: 0.3101, Val Loss: 0.2620, Grad Norm: 1.3543\n",
      "Epoch 149/2001, Time Elapsed: 368.41s, Train Loss: 0.3082, Val Loss: 0.2606, Grad Norm: 1.3491\n",
      "Epoch 150/2001, Time Elapsed: 370.93s, Train Loss: 0.3083, Val Loss: 0.2612, Grad Norm: 1.3484\n",
      "Epoch 151/2001, Time Elapsed: 373.17s, Train Loss: 0.3088, Val Loss: 0.2631, Grad Norm: 1.3505\n",
      "Epoch 152/2001, Time Elapsed: 375.42s, Train Loss: 0.3051, Val Loss: 0.2564, Grad Norm: 1.3424\n",
      "Epoch 153/2001, Time Elapsed: 377.92s, Train Loss: 0.3038, Val Loss: 0.2588, Grad Norm: 1.3359\n",
      "Epoch 154/2001, Time Elapsed: 380.16s, Train Loss: 0.3047, Val Loss: 0.2575, Grad Norm: 1.3419\n",
      "Epoch 155/2001, Time Elapsed: 382.39s, Train Loss: 0.3042, Val Loss: 0.2541, Grad Norm: 1.3409\n",
      "Epoch 156/2001, Time Elapsed: 384.91s, Train Loss: 0.3019, Val Loss: 0.2553, Grad Norm: 1.3316\n",
      "Epoch 157/2001, Time Elapsed: 387.15s, Train Loss: 0.3017, Val Loss: 0.2525, Grad Norm: 1.3328\n",
      "Epoch 158/2001, Time Elapsed: 389.66s, Train Loss: 0.3007, Val Loss: 0.2496, Grad Norm: 1.3312\n",
      "Epoch 159/2001, Time Elapsed: 392.17s, Train Loss: 0.2981, Val Loss: 0.2516, Grad Norm: 1.3253\n",
      "Epoch 160/2001, Time Elapsed: 394.43s, Train Loss: 0.2992, Val Loss: 0.2529, Grad Norm: 1.3284\n",
      "Epoch 161/2001, Time Elapsed: 396.68s, Train Loss: 0.2981, Val Loss: 0.2527, Grad Norm: 1.3241\n",
      "Epoch 162/2001, Time Elapsed: 398.93s, Train Loss: 0.2978, Val Loss: 0.2475, Grad Norm: 1.3278\n",
      "Epoch 163/2001, Time Elapsed: 401.43s, Train Loss: 0.2952, Val Loss: 0.2515, Grad Norm: 1.3168\n",
      "Epoch 164/2001, Time Elapsed: 403.69s, Train Loss: 0.2949, Val Loss: 0.2472, Grad Norm: 1.3176\n",
      "Epoch 165/2001, Time Elapsed: 406.20s, Train Loss: 0.2939, Val Loss: 0.2457, Grad Norm: 1.3158\n",
      "Epoch 166/2001, Time Elapsed: 408.71s, Train Loss: 0.2928, Val Loss: 0.2414, Grad Norm: 1.3129\n",
      "Epoch 167/2001, Time Elapsed: 411.21s, Train Loss: 0.2926, Val Loss: 0.2460, Grad Norm: 1.3102\n",
      "Epoch 168/2001, Time Elapsed: 413.46s, Train Loss: 0.2911, Val Loss: 0.2460, Grad Norm: 1.3067\n",
      "Epoch 169/2001, Time Elapsed: 415.71s, Train Loss: 0.2897, Val Loss: 0.2450, Grad Norm: 1.3040\n",
      "Epoch 170/2001, Time Elapsed: 417.95s, Train Loss: 0.2895, Val Loss: 0.2459, Grad Norm: 1.3043\n",
      "Epoch 171/2001, Time Elapsed: 420.19s, Train Loss: 0.2876, Val Loss: 0.2432, Grad Norm: 1.2987\n",
      "Epoch 172/2001, Time Elapsed: 422.44s, Train Loss: 0.2882, Val Loss: 0.2454, Grad Norm: 1.3044\n",
      "Epoch 173/2001, Time Elapsed: 424.68s, Train Loss: 0.2874, Val Loss: 0.2430, Grad Norm: 1.2803\n",
      "Epoch 174/2001, Time Elapsed: 426.93s, Train Loss: 0.2855, Val Loss: 0.2389, Grad Norm: 1.2747\n",
      "Epoch 175/2001, Time Elapsed: 429.44s, Train Loss: 0.2861, Val Loss: 0.2408, Grad Norm: 1.2794\n",
      "Epoch 176/2001, Time Elapsed: 431.68s, Train Loss: 0.2844, Val Loss: 0.2355, Grad Norm: 1.2702\n",
      "Epoch 177/2001, Time Elapsed: 434.19s, Train Loss: 0.2853, Val Loss: 0.2419, Grad Norm: 1.2786\n",
      "Epoch 178/2001, Time Elapsed: 436.44s, Train Loss: 0.2858, Val Loss: 0.2384, Grad Norm: 1.2801\n",
      "Epoch 179/2001, Time Elapsed: 438.68s, Train Loss: 0.2859, Val Loss: 0.2385, Grad Norm: 1.2792\n",
      "Epoch 180/2001, Time Elapsed: 440.92s, Train Loss: 0.2836, Val Loss: 0.2396, Grad Norm: 1.2730\n",
      "Epoch 181/2001, Time Elapsed: 443.17s, Train Loss: 0.2828, Val Loss: 0.2398, Grad Norm: 1.2682\n",
      "Epoch 182/2001, Time Elapsed: 445.41s, Train Loss: 0.2833, Val Loss: 0.2359, Grad Norm: 1.2736\n",
      "Epoch 183/2001, Time Elapsed: 447.65s, Train Loss: 0.2809, Val Loss: 0.2370, Grad Norm: 1.2555\n",
      "Epoch 184/2001, Time Elapsed: 449.89s, Train Loss: 0.2827, Val Loss: 0.2336, Grad Norm: 1.2634\n",
      "Epoch 185/2001, Time Elapsed: 452.38s, Train Loss: 0.2827, Val Loss: 0.2392, Grad Norm: 1.2620\n",
      "Epoch 186/2001, Time Elapsed: 454.62s, Train Loss: 0.2828, Val Loss: 0.2364, Grad Norm: 1.2610\n",
      "Epoch 187/2001, Time Elapsed: 456.86s, Train Loss: 0.2813, Val Loss: 0.2363, Grad Norm: 1.2583\n",
      "Epoch 188/2001, Time Elapsed: 459.10s, Train Loss: 0.2799, Val Loss: 0.2341, Grad Norm: 1.2527\n",
      "Epoch 189/2001, Time Elapsed: 461.34s, Train Loss: 0.2801, Val Loss: 0.2362, Grad Norm: 1.2517\n",
      "Epoch 190/2001, Time Elapsed: 463.58s, Train Loss: 0.2799, Val Loss: 0.2349, Grad Norm: 1.2522\n",
      "Epoch 191/2001, Time Elapsed: 465.82s, Train Loss: 0.2808, Val Loss: 0.2324, Grad Norm: 1.2525\n",
      "Epoch 192/2001, Time Elapsed: 468.32s, Train Loss: 0.2792, Val Loss: 0.2387, Grad Norm: 1.2469\n",
      "Epoch 193/2001, Time Elapsed: 470.56s, Train Loss: 0.2807, Val Loss: 0.2324, Grad Norm: 1.2529\n",
      "Epoch 194/2001, Time Elapsed: 473.07s, Train Loss: 0.2798, Val Loss: 0.2356, Grad Norm: 1.2485\n",
      "Epoch 195/2001, Time Elapsed: 475.32s, Train Loss: 0.2799, Val Loss: 0.2343, Grad Norm: 1.2503\n",
      "Epoch 196/2001, Time Elapsed: 477.57s, Train Loss: 0.2802, Val Loss: 0.2340, Grad Norm: 1.2505\n",
      "Epoch 197/2001, Time Elapsed: 479.82s, Train Loss: 0.2806, Val Loss: 0.2319, Grad Norm: 1.2518\n",
      "Epoch 198/2001, Time Elapsed: 482.37s, Train Loss: 0.2807, Val Loss: 0.2323, Grad Norm: 1.2545\n",
      "Epoch 199/2001, Time Elapsed: 484.61s, Train Loss: 0.2799, Val Loss: 0.2328, Grad Norm: 1.2512\n",
      "Epoch 200/2001, Time Elapsed: 486.86s, Train Loss: 0.2782, Val Loss: 0.2334, Grad Norm: 1.2456\n",
      "Epoch 201/2001, Time Elapsed: 489.11s, Train Loss: 0.2783, Val Loss: 0.2335, Grad Norm: 1.2469\n",
      "Epoch 202/2001, Time Elapsed: 491.36s, Train Loss: 0.2786, Val Loss: 0.2325, Grad Norm: 1.2455\n",
      "Epoch 203/2001, Time Elapsed: 493.61s, Train Loss: 0.2797, Val Loss: 0.2323, Grad Norm: 1.2505\n",
      "Epoch 204/2001, Time Elapsed: 495.86s, Train Loss: 0.2777, Val Loss: 0.2358, Grad Norm: 1.2408\n",
      "Epoch 205/2001, Time Elapsed: 498.11s, Train Loss: 0.2798, Val Loss: 0.2337, Grad Norm: 1.2480\n",
      "Epoch 206/2001, Time Elapsed: 500.36s, Train Loss: 0.2788, Val Loss: 0.2317, Grad Norm: 1.2453\n",
      "Epoch 207/2001, Time Elapsed: 502.86s, Train Loss: 0.2773, Val Loss: 0.2346, Grad Norm: 1.2409\n",
      "Epoch 208/2001, Time Elapsed: 505.11s, Train Loss: 0.2776, Val Loss: 0.2295, Grad Norm: 1.2428\n",
      "Epoch 209/2001, Time Elapsed: 507.76s, Train Loss: 0.2787, Val Loss: 0.2369, Grad Norm: 1.2445\n",
      "Epoch 210/2001, Time Elapsed: 510.01s, Train Loss: 0.2798, Val Loss: 0.2364, Grad Norm: 1.2472\n",
      "Epoch 211/2001, Time Elapsed: 512.25s, Train Loss: 0.2794, Val Loss: 0.2326, Grad Norm: 1.2478\n",
      "Epoch 212/2001, Time Elapsed: 514.50s, Train Loss: 0.2794, Val Loss: 0.2339, Grad Norm: 1.2487\n",
      "Epoch 213/2001, Time Elapsed: 516.75s, Train Loss: 0.2788, Val Loss: 0.2308, Grad Norm: 1.2458\n",
      "Epoch 214/2001, Time Elapsed: 519.00s, Train Loss: 0.2784, Val Loss: 0.2337, Grad Norm: 1.2458\n",
      "Epoch 215/2001, Time Elapsed: 521.24s, Train Loss: 0.2778, Val Loss: 0.2343, Grad Norm: 1.2409\n",
      "Epoch 216/2001, Time Elapsed: 523.49s, Train Loss: 0.2793, Val Loss: 0.2329, Grad Norm: 1.2468\n",
      "Epoch 217/2001, Time Elapsed: 525.73s, Train Loss: 0.2790, Val Loss: 0.2340, Grad Norm: 1.2435\n",
      "Epoch 218/2001, Time Elapsed: 527.98s, Train Loss: 0.2789, Val Loss: 0.2335, Grad Norm: 1.2426\n",
      "Epoch 219/2001, Time Elapsed: 530.22s, Train Loss: 0.2796, Val Loss: 0.2290, Grad Norm: 1.2454\n",
      "Epoch 220/2001, Time Elapsed: 532.75s, Train Loss: 0.2776, Val Loss: 0.2313, Grad Norm: 1.2406\n",
      "Epoch 221/2001, Time Elapsed: 535.00s, Train Loss: 0.2783, Val Loss: 0.2336, Grad Norm: 1.2431\n",
      "Epoch 222/2001, Time Elapsed: 537.24s, Train Loss: 0.2782, Val Loss: 0.2378, Grad Norm: 1.2442\n",
      "Epoch 223/2001, Time Elapsed: 539.48s, Train Loss: 0.2784, Val Loss: 0.2307, Grad Norm: 1.2425\n",
      "Epoch 224/2001, Time Elapsed: 541.74s, Train Loss: 0.2781, Val Loss: 0.2374, Grad Norm: 1.2436\n",
      "Epoch 225/2001, Time Elapsed: 543.98s, Train Loss: 0.2776, Val Loss: 0.2296, Grad Norm: 1.2415\n",
      "Epoch 226/2001, Time Elapsed: 546.24s, Train Loss: 0.2783, Val Loss: 0.2304, Grad Norm: 1.2423\n",
      "Epoch 227/2001, Time Elapsed: 548.48s, Train Loss: 0.2780, Val Loss: 0.2323, Grad Norm: 1.2411\n",
      "Epoch 228/2001, Time Elapsed: 550.73s, Train Loss: 0.2793, Val Loss: 0.2304, Grad Norm: 1.2458\n",
      "Epoch 229/2001, Time Elapsed: 552.98s, Train Loss: 0.2768, Val Loss: 0.2317, Grad Norm: 1.2389\n",
      "Epoch 230/2001, Time Elapsed: 555.22s, Train Loss: 0.2786, Val Loss: 0.2331, Grad Norm: 1.2421\n",
      "Epoch 231/2001, Time Elapsed: 557.46s, Train Loss: 0.2779, Val Loss: 0.2329, Grad Norm: 1.2415\n",
      "Epoch 232/2001, Time Elapsed: 559.69s, Train Loss: 0.2780, Val Loss: 0.2302, Grad Norm: 1.2411\n",
      "Epoch 233/2001, Time Elapsed: 561.93s, Train Loss: 0.2767, Val Loss: 0.2322, Grad Norm: 1.2351\n",
      "Epoch 234/2001, Time Elapsed: 564.17s, Train Loss: 0.2777, Val Loss: 0.2322, Grad Norm: 1.2424\n",
      "Epoch 235/2001, Time Elapsed: 566.41s, Train Loss: 0.2785, Val Loss: 0.2296, Grad Norm: 1.2431\n",
      "Epoch 236/2001, Time Elapsed: 568.66s, Train Loss: 0.2793, Val Loss: 0.2290, Grad Norm: 1.2459\n",
      "Epoch 237/2001, Time Elapsed: 571.16s, Train Loss: 0.2778, Val Loss: 0.2355, Grad Norm: 1.2418\n",
      "Epoch 238/2001, Time Elapsed: 573.41s, Train Loss: 0.2769, Val Loss: 0.2316, Grad Norm: 1.2375\n",
      "Epoch 239/2001, Time Elapsed: 575.64s, Train Loss: 0.2781, Val Loss: 0.2324, Grad Norm: 1.2430\n",
      "Epoch 240/2001, Time Elapsed: 577.89s, Train Loss: 0.2784, Val Loss: 0.2347, Grad Norm: 1.2441\n",
      "Epoch 241/2001, Time Elapsed: 580.13s, Train Loss: 0.2781, Val Loss: 0.2299, Grad Norm: 1.2422\n",
      "Epoch 242/2001, Time Elapsed: 582.37s, Train Loss: 0.2778, Val Loss: 0.2316, Grad Norm: 1.2419\n",
      "Epoch 243/2001, Time Elapsed: 584.61s, Train Loss: 0.2769, Val Loss: 0.2314, Grad Norm: 1.2375\n",
      "Epoch 244/2001, Time Elapsed: 586.85s, Train Loss: 0.2781, Val Loss: 0.2353, Grad Norm: 1.2414\n",
      "Epoch 245/2001, Time Elapsed: 589.09s, Train Loss: 0.2775, Val Loss: 0.2309, Grad Norm: 1.2398\n",
      "Epoch 246/2001, Time Elapsed: 591.33s, Train Loss: 0.2769, Val Loss: 0.2322, Grad Norm: 1.2386\n",
      "Epoch 247/2001, Time Elapsed: 593.57s, Train Loss: 0.2783, Val Loss: 0.2305, Grad Norm: 1.2424\n",
      "Epoch 248/2001, Time Elapsed: 595.81s, Train Loss: 0.2783, Val Loss: 0.2341, Grad Norm: 1.2409\n",
      "Epoch 249/2001, Time Elapsed: 598.06s, Train Loss: 0.2770, Val Loss: 0.2338, Grad Norm: 1.2381\n",
      "Epoch 250/2001, Time Elapsed: 600.29s, Train Loss: 0.2774, Val Loss: 0.2288, Grad Norm: 1.2400\n",
      "Epoch 251/2001, Time Elapsed: 602.81s, Train Loss: 0.2781, Val Loss: 0.2301, Grad Norm: 1.2402\n",
      "Epoch 252/2001, Time Elapsed: 605.05s, Train Loss: 0.2783, Val Loss: 0.2325, Grad Norm: 1.2406\n",
      "Epoch 253/2001, Time Elapsed: 607.29s, Train Loss: 0.2769, Val Loss: 0.2292, Grad Norm: 1.2401\n",
      "Epoch 254/2001, Time Elapsed: 609.53s, Train Loss: 0.2764, Val Loss: 0.2298, Grad Norm: 1.2388\n",
      "Epoch 255/2001, Time Elapsed: 611.77s, Train Loss: 0.2770, Val Loss: 0.2369, Grad Norm: 1.2392\n",
      "Epoch 256/2001, Time Elapsed: 614.02s, Train Loss: 0.2768, Val Loss: 0.2291, Grad Norm: 1.2367\n",
      "Epoch 257/2001, Time Elapsed: 616.25s, Train Loss: 0.2775, Val Loss: 0.2346, Grad Norm: 1.2397\n",
      "Epoch 258/2001, Time Elapsed: 618.50s, Train Loss: 0.2789, Val Loss: 0.2324, Grad Norm: 1.2442\n",
      "Epoch 259/2001, Time Elapsed: 620.73s, Train Loss: 0.2768, Val Loss: 0.2344, Grad Norm: 1.2381\n",
      "Epoch 260/2001, Time Elapsed: 622.98s, Train Loss: 0.2784, Val Loss: 0.2329, Grad Norm: 1.2440\n",
      "Epoch 261/2001, Time Elapsed: 625.22s, Train Loss: 0.2789, Val Loss: 0.2329, Grad Norm: 1.2451\n",
      "Epoch 262/2001, Time Elapsed: 627.47s, Train Loss: 0.2775, Val Loss: 0.2320, Grad Norm: 1.2397\n",
      "Epoch 263/2001, Time Elapsed: 629.70s, Train Loss: 0.2772, Val Loss: 0.2332, Grad Norm: 1.2400\n",
      "Epoch 264/2001, Time Elapsed: 631.94s, Train Loss: 0.2773, Val Loss: 0.2315, Grad Norm: 1.2379\n",
      "Epoch 265/2001, Time Elapsed: 634.18s, Train Loss: 0.2784, Val Loss: 0.2314, Grad Norm: 1.2424\n",
      "Epoch 266/2001, Time Elapsed: 636.43s, Train Loss: 0.2776, Val Loss: 0.2356, Grad Norm: 1.2402\n",
      "Epoch 267/2001, Time Elapsed: 638.67s, Train Loss: 0.2765, Val Loss: 0.2360, Grad Norm: 1.2374\n",
      "Epoch 268/2001, Time Elapsed: 640.91s, Train Loss: 0.2775, Val Loss: 0.2319, Grad Norm: 1.2397\n",
      "Epoch 269/2001, Time Elapsed: 643.15s, Train Loss: 0.2775, Val Loss: 0.2304, Grad Norm: 1.2402\n",
      "Epoch 270/2001, Time Elapsed: 645.40s, Train Loss: 0.2777, Val Loss: 0.2332, Grad Norm: 1.2403\n",
      "Early stopping with best val loss: 0.22878128687540691!\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train_diffusion_prior(\n",
    "                model=diffusion_prior_model,\n",
    "                noise_scheduler=noise_scheduler,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=test_dataloader,\n",
    "                optimizer=diffusion_optimizer,\n",
    "                scheduler=scheduler,\n",
    "                num_unique_users=len(unique_users),\n",
    "                objective=\"noise_pred\",\n",
    "                device=device,\n",
    "                num_epochs=2001,      # Ensure config.num_epochs is defined\n",
    "                patience=20,\n",
    "                savepath=savepath,\n",
    "                return_losses=True,\n",
    "                verbose=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001, Time Elapsed: 2.26s, Train Loss: 0.2101, Val Loss: 0.1565, Grad Norm: 1.3618\n",
      "Epoch 2/2001, Time Elapsed: 4.65s, Train Loss: 0.2088, Val Loss: 0.1529, Grad Norm: 1.3454\n",
      "Epoch 3/2001, Time Elapsed: 7.16s, Train Loss: 0.2068, Val Loss: 0.1546, Grad Norm: 1.3326\n",
      "Epoch 4/2001, Time Elapsed: 9.42s, Train Loss: 0.2061, Val Loss: 0.1528, Grad Norm: 1.3260\n",
      "Epoch 5/2001, Time Elapsed: 11.93s, Train Loss: 0.2065, Val Loss: 0.1483, Grad Norm: 1.3230\n",
      "Epoch 6/2001, Time Elapsed: 14.45s, Train Loss: 0.2057, Val Loss: 0.1524, Grad Norm: 1.3188\n",
      "Epoch 7/2001, Time Elapsed: 16.72s, Train Loss: 0.2037, Val Loss: 0.1507, Grad Norm: 1.3096\n",
      "Epoch 8/2001, Time Elapsed: 18.99s, Train Loss: 0.2031, Val Loss: 0.1474, Grad Norm: 1.3023\n",
      "Epoch 9/2001, Time Elapsed: 21.53s, Train Loss: 0.2038, Val Loss: 0.1533, Grad Norm: 1.3042\n",
      "Epoch 10/2001, Time Elapsed: 23.79s, Train Loss: 0.2031, Val Loss: 0.1524, Grad Norm: 1.3004\n",
      "Epoch 11/2001, Time Elapsed: 26.05s, Train Loss: 0.2023, Val Loss: 0.1473, Grad Norm: 1.2931\n",
      "Epoch 12/2001, Time Elapsed: 28.57s, Train Loss: 0.2020, Val Loss: 0.1480, Grad Norm: 1.2912\n",
      "Epoch 13/2001, Time Elapsed: 30.81s, Train Loss: 0.2012, Val Loss: 0.1481, Grad Norm: 1.2868\n",
      "Epoch 14/2001, Time Elapsed: 33.04s, Train Loss: 0.2020, Val Loss: 0.1492, Grad Norm: 1.2904\n",
      "Epoch 15/2001, Time Elapsed: 35.29s, Train Loss: 0.2015, Val Loss: 0.1504, Grad Norm: 1.2857\n",
      "Epoch 16/2001, Time Elapsed: 37.53s, Train Loss: 0.2014, Val Loss: 0.1492, Grad Norm: 1.2839\n",
      "Epoch 17/2001, Time Elapsed: 39.76s, Train Loss: 0.2012, Val Loss: 0.1450, Grad Norm: 1.2814\n",
      "Epoch 18/2001, Time Elapsed: 42.26s, Train Loss: 0.2003, Val Loss: 0.1444, Grad Norm: 1.2776\n",
      "Epoch 19/2001, Time Elapsed: 44.78s, Train Loss: 0.1993, Val Loss: 0.1477, Grad Norm: 1.2749\n",
      "Epoch 20/2001, Time Elapsed: 47.03s, Train Loss: 0.1999, Val Loss: 0.1480, Grad Norm: 1.2753\n",
      "Epoch 21/2001, Time Elapsed: 49.29s, Train Loss: 0.1991, Val Loss: 0.1446, Grad Norm: 1.2716\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_loss, val_loss = \u001b[43mtrain_diffusion_prior\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiffusion_prior_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiffusion_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                \u001b[49m\u001b[43mnum_unique_users\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munique_users\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnoise-pred\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Ensure config.num_epochs is defined\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                \u001b[49m\u001b[43msavepath\u001b[49m\u001b[43m=\u001b[49m\u001b[43msavepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreturn_losses\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/REBECA/notebooks/../train_priors.py:125\u001b[39m, in \u001b[36mtrain_diffusion_prior\u001b[39m\u001b[34m(model, noise_scheduler, train_dataloader, val_dataloader, optimizer, scheduler, cfg_drop_prob, num_unique_users, objective, device, num_epochs, patience, savepath, return_losses, verbose)\u001b[39m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid objective. Choose \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnoise-pred\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mz0-pred\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mv_prediction\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m optimizer.step()\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Compute gradient norms for all parameters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rebeca-env/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rebeca-env/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/rebeca-env/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train_diffusion_prior(\n",
    "                model=diffusion_prior_model,\n",
    "                noise_scheduler=noise_scheduler,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=test_dataloader,\n",
    "                optimizer=diffusion_optimizer,\n",
    "                scheduler=scheduler,\n",
    "                num_unique_users=len(unique_users),\n",
    "                objective=\"noise-pred\",\n",
    "                device=device,\n",
    "                num_epochs=2001,      # Ensure config.num_epochs is defined\n",
    "                patience=20,\n",
    "                savepath=savepath,\n",
    "                return_losses=True,\n",
    "                verbose=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or we may run large-scale experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "set_seeds(0)\n",
    "\n",
    "d = image_features.shape[-1]\n",
    "norms = image_features.norm(dim=-1, keepdim=True)\n",
    "norms = torch.clamp(norms, min=1e-8)\n",
    "image_features_normed = image_features / norms * math.sqrt(d)\n",
    "emb_final  = torch.clamp(image_features_normed, -3.2, 3.2) / 3.2   \n",
    "\n",
    "expanded_features = map_embeddings_to_ratings(emb_final, ratings_df)\n",
    "batch_size = 64\n",
    "samples_per_user = 80\n",
    "learning_rate = 1e-5\n",
    "unique_users = filtered_ratings_df[\"worker_id\"].unique()\n",
    "train_dataset = EmbeddingsDataset(\n",
    "        train_df,\n",
    "        image_embeddings=expanded_features[train_df.original_index]\n",
    "    )\n",
    "val_dataset = EmbeddingsDataset(\n",
    "        val_df,\n",
    "        image_embeddings=expanded_features[val_df.original_index]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'timesteps': [1000,2000],\n",
    "    'layers': [8, 16],\n",
    "    'heads': [16, 32],\n",
    "    'dim_feedforward':[2048],\n",
    "    'num_image_tokens': [1, 2],\n",
    "    'num_user_tokens': [4, 8],\n",
    "    'learning_rate': [1e-4, 1e-5],\n",
    "    #'optimizers': ['adamw', 'sgd'],\n",
    "    'optimizers': ['adamw'],\n",
    "    #'schedulers': ['reduce_on_plateau', 'cosine'],\n",
    "    'schedulers': ['reduce_on_plateau'],\n",
    "    'batch_size': [64],\n",
    "    'noise_schedule': [\"laplace\", \"linear\", \"squaredcos_cap_v2\"],\n",
    "    'samples_per_user': [80, 130, 200, 300, 400, 500],\n",
    "    'clip_sample': [False, True],\n",
    "    'rescale_betas': [False],\n",
    "    'objective':[\"v_prediction\", \"noise-pred\"],\n",
    "    'use_ue': [True],\n",
    "    'img_embed_dim': [1024]\n",
    "}\n",
    "\n",
    "savedir = \"../data/flickr/evaluation/diffusion_priors/models/weights/experiment_old_architecture_new_norm\"\n",
    "#savedir = \"../data/flickr/evaluation/diffusion_priors/models/weights/experiment_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists(\"../data/flickr/evaluation/diffusion_priors/models/weights/experiment_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter combinations:   0%|          | 0/4608 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/rebeca-env/lib/python3.13/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running configuration: timesteps=1000, layers=8, heads=16, image_tokens=1, user_tokens=4, learning_rate=0.0001, clip_sample=False, rescale_betas=False, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=laplace, samples_per_user=80, objective=v_prediction, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter combinations:   0%|          | 1/4608 [08:21<642:12:12, 501.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.15440688530604044!\n",
      "Running configuration: timesteps=1000, layers=8, heads=16, image_tokens=1, user_tokens=4, learning_rate=0.0001, clip_sample=False, rescale_betas=False, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=laplace, samples_per_user=80, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/rebeca-env/lib/python3.13/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "Hyperparameter combinations:   0%|          | 2/4608 [15:29<585:57:24, 457.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.11446110159158707!\n",
      "Running configuration: timesteps=1000, layers=8, heads=16, image_tokens=1, user_tokens=4, learning_rate=0.0001, clip_sample=True, rescale_betas=False, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=laplace, samples_per_user=80, objective=v_prediction, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/rebeca-env/lib/python3.13/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "Hyperparameter combinations:   0%|          | 3/4608 [24:53<648:07:22, 506.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.14889527161916097!\n",
      "Running configuration: timesteps=1000, layers=8, heads=16, image_tokens=1, user_tokens=4, learning_rate=0.0001, clip_sample=True, rescale_betas=False, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=laplace, samples_per_user=80, objective=noise-pred, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/rebeca-env/lib/python3.13/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "Hyperparameter combinations:   0%|          | 4/4608 [32:13<614:20:58, 480.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping with best val loss: 0.11399615307648976!\n",
      "Running configuration: timesteps=1000, layers=8, heads=16, image_tokens=1, user_tokens=4, learning_rate=0.0001, clip_sample=False, rescale_betas=False, optimizer=adamw, scheduler=reduce_on_plateau, batch_size=64, noise_schedule=laplace, samples_per_user=130, objective=v_prediction, use_ue=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/anaconda3/envs/rebeca-env/lib/python3.13/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "Hyperparameter combinations:   0%|          | 4/4608 [46:19<888:40:42, 694.88s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msavedir\u001b[49m\u001b[43m=\u001b[49m\u001b[43msavedir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43munique_users\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munique_users\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/REBECA/notebooks/../grid_search.py:91\u001b[39m, in \u001b[36mrun_grid_search\u001b[39m\u001b[34m(train_df, train_dataset, val_dataset, param_grid, savedir, unique_users)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m     90\u001b[39m start = time()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m train_loss, val_loss = \u001b[43mtrain_diffusion_prior\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Ensure config.num_epochs is defined\u001b[39;49;00m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43msavepath\u001b[49m\u001b[43m=\u001b[49m\u001b[43msavepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_losses\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m end = time()\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Evaluate and save results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/REBECA/notebooks/../train_priors.py:129\u001b[39m, in \u001b[36mtrain_diffusion_prior\u001b[39m\u001b[34m(model, noise_scheduler, train_dataloader, val_dataloader, optimizer, scheduler, cfg_drop_prob, num_unique_users, objective, device, num_epochs, patience, savepath, return_losses, verbose)\u001b[39m\n\u001b[32m    126\u001b[39m optimizer.step()\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Compute gradient norms for all parameters\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m total_grad_norm += \u001b[38;5;28msum\u001b[39m(\n\u001b[32m    131\u001b[39m     p.grad.norm(\u001b[32m2\u001b[39m).item() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model.parameters() \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m )\n\u001b[32m    134\u001b[39m global_step += \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "run_grid_search(\n",
    "    train_df=train_df,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    param_grid=param_grid,\n",
    "    savedir=savedir,\n",
    "    unique_users = len(unique_users)\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rebeca-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
